@article{andersen2020a,
  author   = {Andersen, Martin S. and Chen, Tianshi},
  title    = {Smoothing Splines and Rank Structured Matrices: Revisiting the Spline Kernel},
  journal  = {SIAM Journal on Matrix Analysis and Applications},
  volume   = {41},
  number   = {2},
  pages    = {389-412},
  year     = {2020},
  doi      = {10.1137/19M1267349},
  url      = { 
              
              https://doi.org/10.1137/19M1267349
              
              
              
              },
  eprint   = { 
              
              https://doi.org/10.1137/19M1267349
              
              
              
              },
  abstract = { We show that the spline kernel of order \$p\$ is a so-called semiseparable function with semiseparability rank \$p\$. A consequence of this is that kernel matrices generated by the spline kernel are rank structured matrices that can be stored and factorized efficiently. We use this insight to derive new recursive algorithms with linear complexity in the number of knots for various kernel matrix computations. We also discuss applications of these algorithms, including smoothing spline regression, Gaussian process regression, and some related hyperparameter estimation problems. }
}

@article{BEHMANESH2015360,
  title    = {Hierarchical Bayesian model updating for structural identification},
  journal  = {Mechanical Systems and Signal Processing},
  volume   = {64-65},
  pages    = {360-376},
  year     = {2015},
  issn     = {0888-3270},
  doi      = {https://doi.org/10.1016/j.ymssp.2015.03.026},
  url      = {https://www.sciencedirect.com/science/article/pii/S0888327015001545},
  author   = {Iman Behmanesh and Babak Moaveni and Geert Lombaert and Costas Papadimitriou},
  keywords = {Hierarchical Bayesian model updating, Damage identification, Uncertainty quantification, Continuous structural health monitoring, Prediction error correlation, Environmental condition effects}
}

@article{bosch2023a,
  author    = {Bosch, Nathanael and Hennig, Philipp and Tronarp, Filip},
  title     = {Probabilistic Exponential Integrators},
  format    = {article},
  journal   = {Advances in Neural Information Processing Systems},
  volume    = {36},
  year      = {2023},
  publisher = {Neural information processing systems foundation}
}

@inproceedings{brincker2006understanding,
  title        = {Understanding stochastic subspace identification},
  author       = {Brincker, Rune and Andersen, Palle},
  booktitle    = {Conference Proceedings: IMAC-XXIV: A Conference \& Exposition on Structural Dynamics},
  year         = {2006},
  organization = {Society for Experimental Mechanics}
}

@article{BULL2021107141,
  title    = {Foundations of population-based SHM, Part I: Homogeneous populations and forms},
  journal  = {Mechanical Systems and Signal Processing},
  volume   = {148},
  pages    = {107141},
  year     = {2021},
  issn     = {0888-3270},
  doi      = {https://doi.org/10.1016/j.ymssp.2020.107141},
  url      = {https://www.sciencedirect.com/science/article/pii/S0888327020305276},
  author   = {L.A. Bull and P.A. Gardner and J. Gosliga and T.J. Rogers and N. Dervilis and E.J. Cross and E. Papatheou and A.E. Maguire and C. Campos and K. Worden},
  keywords = {Population-based structural health monitoring, Pattern recognition, Wind turbine monitoring}
}

@article{chen2021a,
  author    = {Chen, Tianshi and Andersen, Martin S.},
  title     = {On semiseparable kernels and efficient implementation for regularized system identification and function estimation},
  language  = {eng},
  format    = {article},
  journal   = {Automatica},
  volume    = {132},
  pages     = {109682},
  year      = {2021},
  issn      = {18732836, 00051098},
  publisher = {Elsevier Ltd},
  doi       = {10.1016/j.automatica.2021.109682}
}

@article{dao2024a,
  author    = {Dao, Tri and Gu, Albert},
  title     = {Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality},
  language  = {eng},
  format    = {article},
  journal   = {Arxiv (cornell University)},
  year      = {2024},
  publisher = {Cornell University},
  doi       = {10.48550/arXiv.2405.21060}
}

@article{dempster2020a,
  author    = {Dempster, Angus and Petitjean, François and Webb, Geoffrey I.},
  title     = {ROCKET: exceptionally fast and accurate time series classification using random convolutional kernels},
  language  = {eng},
  format    = {article},
  journal   = {Data Mining and Knowledge Discovery},
  volume    = {34},
  number    = {5},
  pages     = {1454-1495},
  year      = {2020},
  issn      = {1573756x, 13845810},
  publisher = {Springer},
  doi       = {10.1007/s10618-020-00701-z}
}


@article{dempster2021a,
  author    = {Dempster, Angus and Schmidt, Daniel F. and Webb, Geoffrey I.},
  title     = {MiniRocket: A Very Fast (Almost) Deterministic Transform for Time Series Classification},
  language  = {eng},
  format    = {article},
  journal   = {Proceedings of the Acm Sigkdd International Conference on Knowledge Discovery and Data Mining},
  pages     = {248-257},
  year      = {2021},
  isbn      = {1450383327, 9781450383325},
  publisher = {Association for Computing Machinery},
  doi       = {10.1145/3447548.3467231}
}

@misc{fno2021,
  title         = {Fourier Neural Operator for Parametric Partial Differential Equations},
  author        = {Zongyi Li and Nikola Kovachki and Kamyar Azizzadenesheli and Burigede Liu and Kaushik Bhattacharya and Andrew Stuart and Anima Anandkumar},
  year          = {2021},
  eprint        = {2010.08895},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2010.08895}
}

@misc{gu2022efficientlymodelinglongsequences,
  title         = {Efficiently Modeling Long Sequences with Structured State Spaces},
  author        = {Albert Gu and Karan Goel and Christopher Ré},
  year          = {2022},
  eprint        = {2111.00396},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2111.00396}
}

@misc{gu2024mambalineartimesequencemodeling,
  title         = {Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
  author        = {Albert Gu and Tri Dao},
  year          = {2024},
  eprint        = {2312.00752},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2312.00752}
}


@misc{guo2025loglinearattention,
  title         = {Log-Linear Attention},
  author        = {Han Guo and Songlin Yang and Tarushii Goel and Eric P. Xing and Tri Dao and Yoon Kim},
  year          = {2025},
  eprint        = {2506.04761},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2506.04761}
}

@article{hartikainen2010a,
  author    = {Hartikainen, Jouni and Särkkä, Simo},
  title     = {Kalman filtering and smoothing solutions to temporal Gaussian process regression models},
  language  = {eng},
  format    = {article},
  journal   = {Proceedings of the 2010 Ieee International Workshop on Machine Learning for Signal Processing, Mlsp 2010},
  pages     = {379-384},
  year      = {2010},
  issn      = {21610363, 15512541},
  isbn      = {1424478758, 9781424478750, 1424478766, 1424478774, 9781424478767, 9781424478774},
  publisher = {IEEE},
  doi       = {10.1109/MLSP.2010.5589113}
}
@article{higham2024a,
  author    = {Higham, Nicholas J.},
  title     = {THE POWER OF BIDIAGONAL MATRICES},
  language  = {eng},
  format    = {article},
  journal   = {Electronic Journal of Linear Algebra},
  volume    = {40},
  pages     = {453-474},
  year      = {2024},
  issn      = {10813810, 15379582},
  publisher = {International Linear Algebra Society},
  doi       = {10.13001/ela.2024.8297}
}

@article{ismail-fawaz2023a,
  author    = {Ismail-Fawaz, Ali and Devanne, Maxime and Berretti, Stefano and Weber, Jonathan and Forestier, Germain},
  title     = {LITE: Light Inception with boosTing tEchniques for Time Series Classification},
  language  = {eng},
  format    = {article},
  journal   = {2023 Ieee 10th International Conference on Data Science and Advanced Analytics, Dsaa 2023 - Proceedings},
  pages     = {1-10},
  year      = {2023},
  isbn      = {9798350345032},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
  doi       = {10.1109/DSAA60987.2023.10302569}
}

@article{ismail-fawaz2024a,
  author    = {Ismail-Fawaz, Ali and Devanne, Maxime and Berretti, Stefano and Weber, Jonathan and Forestier, Germain},
  title     = {Look Into the LITE in Deep Learning for Time Series Classification},
  language  = {eng},
  format    = {article},
  journal   = {Arxiv (cornell University)},
  year      = {2024},
  publisher = {Cornell University},
  doi       = {10.48550/arXiv.2409.02869}
}

@article{katharopoulos2020a,
  author   = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, François},
  title    = {Transformers are RNNs: Fast Autoregressive Transformers with Linear
              Attention},
  language = {und},
  format   = {article},
  year     = {2020}
}


@article{kersting2020a,
  author    = {Kersting, Hans and Sullivan, T. J. and Hennig, Philipp},
  title     = {Convergence rates of Gaussian ODE filters},
  language  = {eng},
  format    = {article},
  journal   = {Statistics and Computing},
  volume    = {30},
  number    = {6},
  pages     = {1791-1816},
  year      = {2020},
  issn      = {15731375, 09603174},
  publisher = {Springer},
  doi       = {10.1007/s11222-020-09972-4}
}

@article{lathourakis2024a,
  author    = {Lathourakis, Christos and Cicirello, Alice},
  title     = {Physics enhanced sparse identification of dynamical systems with discontinuous nonlinearities},
  language  = {eng},
  format    = {article},
  journal   = {Nonlinear Dynamics},
  volume    = {112},
  number    = {13},
  pages     = {11237-11264},
  year      = {2024},
  issn      = {1573269x, 0924090x},
  publisher = {Springer Netherlands},
  doi       = {10.1007/s11071-024-09652-2}
}

@article{lu2021a,
  author    = {Lu, Lu and Jin, Pengzhan and Pang, Guofei and Zhang, Zhongqiang and Karniadakis, George Em},
  title     = {Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators},
  language  = {eng},
  format    = {article},
  journal   = {Nature Machine Intelligence},
  volume    = {3},
  number    = {3},
  pages     = {218-229},
  year      = {2021},
  issn      = {25225839},
  publisher = {Nature Publishing Group UK},
  doi       = {10.1038/s42256-021-00302-5}
}

@article{mclenny2023a,
  title    = {Self-adaptive physics-informed neural networks},
  journal  = {Journal of Computational Physics},
  volume   = {474},
  pages    = {111722},
  year     = {2023},
  issn     = {0021-9991},
  doi      = {https://doi.org/10.1016/j.jcp.2022.111722},
  url      = {https://www.sciencedirect.com/science/article/pii/S0021999122007859},
  author   = {Levi D. McClenny and Ulisses M. Braga-Neto},
  keywords = {Physics-informed neural networks, Scientific machine learning, Numerical methods for PDE},
  abstract = {Physics-Informed Neural Networks (PINNs) have emerged recently as a promising application of deep neural networks to the numerical solution of nonlinear partial differential equations (PDEs). However, it has been recognized that adaptive procedures are needed to force the neural network to fit accurately the stubborn spots in the solution of “stiff” PDEs. In this paper, we propose a fundamentally new way to train PINNs adaptively, where the adaptation weights are fully trainable and applied to each training point individually, so the neural network learns autonomously which regions of the solution are difficult and is forced to focus on them. The self-adaptation weights specify a soft multiplicative soft attention mask, which is reminiscent of similar mechanisms used in computer vision. The basic idea behind these SA-PINNs is to make the weights increase as the corresponding losses increase, which is accomplished by training the network to simultaneously minimize the losses and maximize the weights. In addition, we show how to build a continuous map of self-adaptive weights using Gaussian Process regression, which allows the use of stochastic gradient descent in problems where conventional gradient descent is not enough to produce accurate solutions. Finally, we derive the Neural Tangent Kernel matrix for SA-PINNs and use it to obtain a heuristic understanding of the effect of the self-adaptive weights on the dynamics of training in the limiting case of infinitely-wide PINNs, which suggests that SA-PINNs work by producing a smooth equalization of the eigenvalues of the NTK matrix corresponding to the different loss terms. In numerical experiments with several linear and nonlinear benchmark problems, the SA-PINN outperformed other state-of-the-art PINN algorithm in L2 error, while using a smaller number of training epochs.}
}

@article{meurant1992a,
  author    = {MEURANT, G},
  title     = {A REVIEW ON THE INVERSE OF SYMMETRICAL TRIDIAGONAL AND BLOCK TRIDIAGONAL MATRICES},
  language  = {eng},
  format    = {article},
  journal   = {Siam Journal on Matrix Analysis and Applications},
  volume    = {13},
  number    = {3},
  pages     = {707-728},
  year      = {1992},
  issn      = {10957162, 08954798},
  publisher = {SIAM PUBLICATIONS},
  doi       = {10.1137/0613045}
}

@article{pal2025a,
  author   = {Pal, Avik and Edelman, Alan and Rackauckas, Christopher},
  title    = {Semi-Explicit Neural DAEs: Learning Long-Horizon Dynamical Systems with Algebraic Constraints},
  language = {eng},
  format   = {article},
  year     = {2025},
  doi      = {10.48550/arXiv.2505.20515}
}

@article{peeters1999reference,
  title     = {Reference-based stochastic subspace identification for output-only modal analysis},
  author    = {Peeters, Bart and De Roeck, Guido},
  journal   = {Mechanical systems and signal processing},
  volume    = {13},
  number    = {6},
  pages     = {855--878},
  year      = {1999},
  publisher = {Elsevier}
}

@article{rackauckas2021a,
  author    = {Rackauckas, Christopher and Ma, Yingbo and Martensen, Julius and Warner, Collin and Zubov, Kirill and Supekar, Rohit and Skinner, Dominic and Ramadhan, Ali and Edelman, Alan},
  title     = {Universal Differential Equations for Scientific Machine Learning},
  language  = {und},
  format    = {article},
  journal   = {Research Square (research Square)},
  pages     = {29},
  year      = {2021},
  publisher = {Research Square},
  doi       = {10.21203/rs.3.rs-55125/v1}
}


@article{raissi2019a,
  author    = {Raissi, M. and Perdikaris, P. and Karniadakis, G. E.},
  title     = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
  language  = {eng},
  format    = {article},
  journal   = {Journal of Computational Physics},
  volume    = {378},
  pages     = {686-707},
  year      = {2019},
  issn      = {10902716, 00219991},
  publisher = {Academic Press Inc.},
  doi       = {10.1016/j.jcp.2018.10.045}
}

@article{SEDEHI2022109296,
  title    = {Hierarchical Bayesian uncertainty quantification of Finite Element models using modal statistical information},
  journal  = {Mechanical Systems and Signal Processing},
  volume   = {179},
  pages    = {109296},
  year     = {2022},
  issn     = {0888-3270},
  doi      = {https://doi.org/10.1016/j.ymssp.2022.109296},
  url      = {https://www.sciencedirect.com/science/article/pii/S088832702200437X},
  author   = {Omid Sedehi and Costas Papadimitriou and Lambros S. Katafygiotis},
  keywords = {Model updating, Bayesian methods, Hierarchical models, Uncertainty quantification, Modal data}
}

@article{sindy2016a,
  author   = {Steven L. Brunton  and Joshua L. Proctor  and J. Nathan Kutz },
  title    = {Discovering governing equations from data by sparse identification of nonlinear dynamical systems},
  journal  = {Proceedings of the National Academy of Sciences},
  volume   = {113},
  number   = {15},
  pages    = {3932-3937},
  year     = {2016},
  doi      = {10.1073/pnas.1517384113},
  url      = {https://www.pnas.org/doi/abs/10.1073/pnas.1517384113},
  eprint   = {https://www.pnas.org/doi/pdf/10.1073/pnas.1517384113},
  abstract = {Understanding dynamic constraints and balances in nature has facilitated rapid development of knowledge and enabled technology, including aircraft, combustion engines, satellites, and electrical power. This work develops a novel framework to discover governing equations underlying a dynamical system simply from data measurements, leveraging advances in sparsity techniques and machine learning. The resulting models are parsimonious, balancing model complexity with descriptive ability while avoiding overfitting. There are many critical data-driven problems, such as understanding cognition from neural recordings, inferring climate patterns, determining stability of financial markets, predicting and suppressing the spread of disease, and controlling turbulence for greener transportation and energy. With abundant data and elusive laws, data-driven discovery of dynamics will continue to play an important role in these efforts. Extracting governing equations from data is a central challenge in many diverse areas of science and engineering. Data are abundant whereas models often remain elusive, as in climate science, neuroscience, ecology, finance, and epidemiology, to name only a few examples. In this work, we combine sparsity-promoting techniques and machine learning with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only assumption about the structure of the model is that there are only a few important terms that govern the dynamics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to determine the fewest terms in the dynamic governing equations required to accurately represent the data. This results in parsimonious models that balance accuracy with model complexity to avoid overfitting. We demonstrate the algorithm on a wide range of problems, from simple canonical systems, including linear and nonlinear oscillators and the chaotic Lorenz system, to the fluid vortex shedding behind an obstacle. The fluid example illustrates the ability of this method to discover the underlying dynamics of a system that took experts in the community nearly 30 years to resolve. We also show that this method generalizes to parameterized systems and systems that are time-varying or have external forcing.}
}



@inproceedings{smith2023simplified,
  title     = {Simplified State Space Layers for Sequence Modeling},
  author    = {Jimmy T.H. Smith and Andrew Warrington and Scott Linderman},
  booktitle = {The Eleventh International Conference on Learning Representations },
  year      = {2023},
  url       = {https://openreview.net/forum?id=Ai8Hw3AXqks}
}

@book{stoica2005spectral,
  title     = {Spectral analysis of signals},
  author    = {Stoica, Petre and Moses, Randolph L and others},
  volume    = {452},
  year      = {2005},
  publisher = {Citeseer}
}

@misc{symbolicregression.jl,
  title         = {Interpretable Machine Learning for Science with PySR and SymbolicRegression.jl},
  author        = {Miles Cranmer},
  year          = {2023},
  eprint        = {2305.01582},
  archiveprefix = {arXiv},
  primaryclass  = {astro-ph.IM},
  url           = {https://arxiv.org/abs/2305.01582}
}

@article{tan2022a,
  author    = {Tan, Chang Wei and Dempster, Angus and Bergmeir, Christoph and Webb, Geoffrey I.},
  title     = {MultiRocket: multiple pooling operators and transformations for fast and effective time series classification},
  language  = {eng},
  format    = {article},
  journal   = {Data Mining and Knowledge Discovery},
  volume    = {36},
  number    = {5},
  pages     = {1623-1646},
  year      = {2022},
  issn      = {1573756x, 13845810},
  publisher = {Springer},
  doi       = {10.1007/s10618-022-00844-1}
}

@article{wang2024a,
  title    = {Respecting causality for training physics-informed neural networks},
  journal  = {Computer Methods in Applied Mechanics and Engineering},
  volume   = {421},
  pages    = {116813},
  year     = {2024},
  issn     = {0045-7825},
  doi      = {https://doi.org/10.1016/j.cma.2024.116813},
  url      = {https://www.sciencedirect.com/science/article/pii/S0045782524000690},
  author   = {Sifan Wang and Shyam Sankaran and Paris Perdikaris},
  keywords = {Deep learning, Partial differential equations, Computational physics, Chaotic systems},
  abstract = {While the popularity of physics-informed neural networks (PINNs) is steadily rising, to this date PINNs have not been successful in simulating dynamical systems whose solution exhibits multi-scale, chaotic or turbulent behavior. In this work we attribute this shortcoming to the inability of existing PINNs formulations to respect the spatio-temporal causal structure that is inherent to the evolution of physical systems. We argue that this is a fundamental limitation and a key source of error that can ultimately steer PINN models to converge towards erroneous solutions. We address this pathology by proposing a simple re-formulation of PINNs loss functions that can explicitly account for physical causality during model training. We demonstrate that this simple modification alone is enough to introduce significant accuracy improvements, as well as a practical quantitative mechanism for assessing the convergence of a PINNs model. We provide state-of-the-art numerical results across a series of benchmarks for which existing PINNs formulations fail, including the chaotic Lorenz system, the Kuramoto–Sivashinsky equation in the chaotic regime, and the Navier–Stokes equations. To the best of our knowledge, this is the first time that PINNs have been successful in simulating such systems, introducing new opportunities for their applicability to problems of industrial complexity.}
}

@misc{yao2025,
  title         = {Propagating Model Uncertainty through Filtering-based Probabilistic Numerical ODE Solvers},
  author        = {Dingling Yao and Filip Tronarp and Nathanael Bosch},
  year          = {2025},
  eprint        = {2503.04684},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
  url           = {https://arxiv.org/abs/2503.04684}
}