<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Backpropagation is The Adjoint Method | Mikkel Paltorp </title> <meta name="author" content="Mikkel Paltorp"> <meta name="description" content="Backpropagation is often introduced as something developed within the field of machine learning. However, the story is that backpropagation really is just a special case of the adjoint method. This note is in two parts. In the first part we review the adjoint method while in the second part we describe how backpropagation is a special case of the adjoint method with a structure that result in scalable computations algorithms."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%94%AC&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://mipals.github.io/blog/2025/adjoint-method/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Backpropagation is The Adjoint Method",
            "description": "Backpropagation is often introduced as something developed within the field of machine learning. However, the story is that backpropagation really is just a special case of the adjoint method. This note is in two parts. In the first part we review the adjoint method while in the second part we describe how backpropagation is a special case of the adjoint method with a structure that result in scalable computations algorithms.",
            "published": "October 25, 2025",
            "authors": [
              
              {
                "author": "Mikkel Paltorp",
                "authorURL": "https://mipals.github.io",
                "affiliations": [
                  {
                    "name": "Technical University of Denmark",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Mikkel</span> Paltorp </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Backpropagation is The Adjoint Method</h1> <p>Backpropagation is often introduced as something developed within the field of machine learning. However, the story is that backpropagation really is just a special case of the adjoint method. This note is in two parts. In the first part we review the adjoint method while in the second part we describe how backpropagation is a special case of the adjoint method with a structure that result in scalable computations algorithms.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#the-adjoint-method">The Adjoint Method</a> </div> <div> <a href="#backpropagation-and-the-adjoint-method">Backpropagation and the adjoint method</a> </div> </nav> </d-contents> <h1 id="the-adjoint-method">The adjoint method</h1> <p>In many engineering applications we are interested in solving equality constrained optimization problems where the optimization variable ($\theta$) implicitly defines another variable ($u$) that then appears in the objective function<d-footnote>A famous example of this is [topology optimization](https://en.wikipedia.org/wiki/Topology_optimization).</d-footnote>. Written out this means solving optimization problems of the following form</p> \[\begin{equation} \begin{aligned} \min_{\theta \in \mathbb{R}^{n_\theta}} \quad &amp; L(u,\theta), \quad\ \quad\ \left(\text{some objective function}\right)\\ \text{subject to }\quad &amp; f(u,\theta) = 0, \quad \left(\text{some equality constraint}\right) \\ &amp; u \in \mathbb{R}^{n_u}. \end{aligned} \end{equation}\] <p>Most methods for solving equality constrained optimization problems requires the computation of gradient of the objective function with respect to the optimization variable $\theta$. This is where the problem comes in: Naively computing the gradient requires the computation of the sensitivities of the implicitly defined variable $u$ with respect to the optimization variable $\theta$ $\left(\text{i.e.}\ \frac{\mathrm{d}u}{\mathrm{d}\theta} \in \mathbb{R}^{n_u \times n_\theta}\right)$. This result in a computational bottleneck as forming the sensitivities scales as $\mathcal{O}(n_un_\theta)$, meaning that adding a new parameter adds $n_u$ additional sensitivities (and similarly adding one more variable would add $n_\theta$ additional sensitivities).</p> <p>To resolve this computational bottleneck the adjoint method was introduced. The first step in the derivation of the adjoint method is to introduce the Lagrangian of the objective function, i.e.</p> \[\begin{equation} \mathcal{L}(u,\theta,\lambda) = L(u,\theta) + \lambda^\top f(u,\theta). \end{equation}\] <p>It is easy to see that whenever $u$ satisfy the equality constraint $f(u,\theta)=0$, then the Lagrangian is equal to the original objective function. This in turn mean that the two gradients $\nabla_\theta L(u,\theta,\lambda)$ and $\nabla_\theta \mathcal{L}(u,\theta,\lambda)$ are equal whenever $u$ satisfy the equality constraint $f(u,\theta) = 0$. The reason why the introduction of the additional term in the Lagrangian is useful is that $\lambda$ can be set to be anything. In particular, we aim to set in a way so that we can avoid computing the otherwise computational expensive sensitivities $\frac{\mathrm{d}u}{\mathrm{d}\theta}$. We start by computing the total derivative of the Lagrangian with respect to $\theta$<d-footnote>The total derivative is the transpose of the gradient i.e. $\left(\frac{\mathrm{d}L}{\mathrm{d}\theta}\right)^\top = \nabla_\theta L$. That is multiplying the change in $\theta$ with the total derivative (rather than the transpose of the gradient) gives the change in the function $L$.</d-footnote></p> \[\begin{equation} \frac{\mathrm{d}\mathcal{L}}{\mathrm{d}\theta} = \frac{\partial L}{\partial\theta} + \frac{\partial L}{\partial u}\frac{\mathrm{d}u}{\mathrm{d}\theta} + \lambda^\top\left(\frac{\partial f}{\partial\theta} + \frac{\partial f}{\partial u}\frac{\mathrm{d}u}{\mathrm{d}\theta}\right). \end{equation}\] <p>Now we collect the terms that depend on the undesired $\frac{\mathrm{d}u}{\mathrm{d}\theta}$ result in</p> \[\begin{equation} \frac{\mathrm{d}\mathcal{L}}{\mathrm{d}\theta} = \frac{\partial L}{\partial\theta} + \lambda^\top\frac{\partial f}{\partial\theta} + \left(\frac{\partial L}{\partial u} + \lambda^\top\frac{\partial f}{\partial u}\right)\frac{\mathrm{d}u}{\mathrm{d}\theta} . \end{equation}\] <p>As we can choose $\lambda$ freely a natural idea is to set it in a way such that the term in front of the undesired term vanishes. This means that we can choose $\lambda$ as the solution to the equation</p> \[\begin{equation} \frac{\partial L}{\partial u} + \lambda^\top\frac{\partial f}{\partial u} = 0 \quad \Rightarrow \quad \lambda^\top = -\frac{\partial L}{\partial u}\left(\frac{\partial f}{\partial u}\right)^{-1}. \end{equation}\] <p>Inserting $\lambda^\top$ back into the equation we find that the gradient of the Lagrangian with respect to $\theta$ is given by</p> \[\begin{equation} \frac{\mathrm{d}\mathcal{L}}{\mathrm{d}\theta} = \frac{\partial L}{\partial\theta} \underbrace{- \frac{\partial L}{\partial u}\left(\frac{\partial f}{\partial u}\right)^{-1}}_{\lambda^\top}\frac{\partial f}{\partial\theta}. \end{equation}\] <p>To conclude: The adjoint method is a simple way to avoid the computational bottleneck of computing the sensitivities $\frac{\mathrm{d}u}{\mathrm{d}\theta}$ by cleverly computing the so-called adjoint variable $\lambda$ in a way that eliminates the need to compute the problematic sensitivities.</p> <h3 id="example-linearly-constrained-problem-with-structure">Example: Linearly constrained problem with structure</h3> <p>In order to illustrate the adjoint method we consider a simple linearly constrained problem of the form (inspiration from problem 38 in <d-cite key="bright2025matrixcalculusformachine"></d-cite>)</p> \[\begin{aligned} L(u,\theta) &amp;= \left(c^\top u(\theta)\right)^2, \\ f(u,\theta) &amp;= A(\theta)u - b = 0, \end{aligned}\] <p>where $A(\theta) \in \mathbb{R}^{n\times n}$ is a symmetric tridiagonal matrix that depends on the parameters $\theta \in \mathbb{R}^{2n-1}$ as</p> \[A = \begin{bmatrix} \theta_1 &amp; \theta_{n+1} &amp; &amp; &amp; 0 \\ \theta_{n+1} &amp; \theta_2 &amp; \ddots &amp; &amp; \\ &amp; \ddots &amp; \ddots &amp; \ddots &amp; \\ &amp; &amp; \ddots &amp; \theta_{n-1} &amp; \theta_{2n-1} \\ 0 &amp; &amp; &amp; \theta_{2n-1} &amp; \theta_n \end{bmatrix}.\] <p>Now in order to compute the gradient of interested we start by computing the adjoint variable $\lambda$ as</p> \[\lambda^\top = -\frac{\partial L}{\partial u}\left(\frac{\partial f}{\partial u}\right)^{-1} = -2\left(c^\top u\right)c^\top A(\theta)^{-1}.\] <p>Note that in practice we do not form $A(\theta)^{-1}$ explicitly but rather compute $\lambda$ by solving the linear system $ A(\theta)^\top\lambda = -2c\left(c^\top u\right)$. Using the adjoint variable we can compute the gradient of $L$ with respect to $\theta$ as</p> \[\frac{\mathrm{d}L}{\mathrm{d}\theta} = \underbrace{\frac{\partial L}{\partial\theta}}_{=0} + \lambda^\top\frac{\partial f}{\partial\theta} = \lambda^\top\frac{\partial A}{\partial\theta}u.\] <p>In the concrete case of the derivatives of $A$ with respect to $\theta_i$ we have that</p> \[\frac{\partial A}{\partial \theta_i} = \begin{bmatrix} \delta_{i,1} &amp; \delta_{i,n+1} &amp; &amp; &amp; 0 \\ \delta_{i,n+1} &amp; \delta_{i,2} &amp; \ddots &amp; &amp; \\ &amp; \ddots &amp; \ddots &amp; \ddots &amp; \\ &amp; &amp; \ddots &amp; \delta_{i,n-1} &amp; \delta_{i,2n-1} \\ 0 &amp; &amp; &amp; \delta_{i,2n-1} &amp; \delta_{i,n} \end{bmatrix}.\] <p>Using this it follows that the $i$th component of the gradient can be computed as</p> \[\lambda^\top\frac{\partial A}{\partial \theta_i}u = \begin{cases} \lambda_i u_i, \quad &amp;i \leq n, \\ \lambda_{i+1-n}u_{i-n} + \lambda_{i-n} u_{i+1-n}, \quad &amp;i &gt; n. \end{cases}\] <details><summary>Code</summary> <div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">LinearAlgebra</span> 
<span class="n">n</span> <span class="o">=</span> <span class="mi">1000</span>        <span class="c"># Number of linear constraints</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">rand</span><span class="x">(</span><span class="n">n</span><span class="x">)</span>     <span class="c"># Objective vector</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">rand</span><span class="x">(</span><span class="n">n</span><span class="x">)</span>     <span class="c"># Linear equality vector</span>
<span class="n">θ</span> <span class="o">=</span> <span class="n">rand</span><span class="x">(</span><span class="mi">2</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="x">)</span>  <span class="c"># Parameters of the equality constraint</span>

<span class="c"># Parametrizing linear equality constraint matrix</span>
<span class="n">A</span><span class="x">(</span><span class="n">θ</span><span class="x">)</span> <span class="o">=</span> <span class="kt">SymTridiagonal</span><span class="x">(</span><span class="n">θ</span><span class="x">[</span><span class="mi">1</span><span class="o">:</span><span class="n">n</span><span class="x">],</span> <span class="n">θ</span><span class="x">[</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="o">:</span><span class="k">end</span><span class="x">])</span> 

<span class="c"># Objective function</span>
<span class="n">f</span><span class="x">(</span><span class="n">θ</span><span class="x">)</span> <span class="o">=</span> <span class="x">(</span><span class="n">c</span><span class="err">'</span> <span class="o">*</span> <span class="x">(</span><span class="n">A</span><span class="x">(</span><span class="n">θ</span><span class="x">)</span> <span class="o">\</span> <span class="n">b</span><span class="x">))</span><span class="o">^</span><span class="mi">2</span> 

<span class="c"># Partial derivatives</span>
<span class="n">partial</span><span class="x">(</span><span class="n">i</span><span class="x">,</span><span class="n">n</span><span class="x">,</span><span class="n">λ</span><span class="x">,</span><span class="n">u</span><span class="x">)</span> <span class="o">=</span> <span class="n">i</span> <span class="o">&lt;=</span> <span class="n">n</span> <span class="o">?</span>  <span class="n">λ</span><span class="x">[</span><span class="n">i</span><span class="x">]</span><span class="o">*</span><span class="n">u</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">:</span> <span class="n">λ</span><span class="x">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="o">-</span><span class="n">n</span><span class="x">]</span><span class="o">*</span><span class="n">u</span><span class="x">[</span><span class="n">i</span><span class="o">-</span><span class="n">n</span><span class="x">]</span> <span class="o">+</span> <span class="n">λ</span><span class="x">[</span><span class="n">i</span><span class="o">-</span><span class="n">n</span><span class="x">]</span><span class="o">*</span><span class="n">u</span><span class="x">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="o">-</span><span class="n">n</span><span class="x">]</span>

<span class="c"># Gradient computation</span>
<span class="k">function</span><span class="nf"> ∇f</span><span class="x">(</span><span class="n">θ</span><span class="x">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">length</span><span class="x">(</span><span class="n">b</span><span class="x">)</span>
    <span class="n">M</span> <span class="o">=</span> <span class="n">A</span><span class="x">(</span><span class="n">θ</span><span class="x">)</span>                <span class="c"># Defining constraint matrix</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">M</span> <span class="o">\</span> <span class="n">b</span>               <span class="c"># Computing solution</span>
    <span class="n">λ</span> <span class="o">=</span> <span class="n">M</span> <span class="o">\</span> <span class="x">(</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">c</span><span class="o">*</span><span class="x">(</span><span class="n">c</span><span class="err">'</span><span class="o">*</span><span class="n">u</span><span class="x">))</span>   <span class="c"># Adjoint variables</span>
    <span class="k">return</span> <span class="x">[</span><span class="n">partial</span><span class="x">(</span><span class="n">i</span><span class="x">,</span><span class="n">n</span><span class="x">,</span><span class="n">λ</span><span class="x">,</span><span class="n">u</span><span class="x">)</span> <span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="o">:</span><span class="mi">2</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="x">]</span>
<span class="k">end</span>

<span class="c"># Testing against ForwardDiff</span>
<span class="k">using</span> <span class="n">Test</span><span class="x">,</span> <span class="n">ForwardDiff</span>
<span class="nd">@test</span> <span class="n">ForwardDiff</span><span class="o">.</span><span class="n">gradient</span><span class="x">(</span><span class="n">f</span><span class="x">,</span><span class="n">θ</span><span class="x">)</span> <span class="n">≈</span> <span class="n">∇f</span><span class="x">(</span><span class="n">θ</span><span class="x">)</span>
</code></pre></div></div> </details> <h1 id="backpropagation-and-the-adjoint-method">Backpropagation and the adjoint method</h1> <p>Section 5.5 of <d-cite key="edelman:backprop"></d-cite> includes an example of how the adjoint method and backpropagation of neural networks are similar. In neural networks we are often interested in minimizing a loss function of the form</p> \[L(\theta; u_0) = \| \Phi_N(\theta; u_0) - y \|_2^2 + \mu \mathcal{R}(\theta), \quad \theta \in \mathbb{R}^k, \quad u_0 \in \mathbb{R}^m, \quad y \in \mathbb{R}^n,\] <p>where the notation “$;u_0$” is used in order to highlight that $u_0$ is a constant input (i.e. most often the “data”) and $\mathcal{R}(\theta)$ is some regularization function (e.g. $\mathcal{R}(\theta) = \Vert\theta\Vert_2^2$). Furthermore $\Phi_N(\theta;u_0): \mathbb{R}^k \to \mathbb{R}^n$ is a neural network with $N$ layers that given a set of constant inputs $u_0$ maps the parameters $\theta$ to an output. Now, the above objective functions does not include any equality constraints. However, one should realize that the a $N$-layer neural network is nothing more than a series of composition of $N$ functions</p> \[\Phi_N(\theta; u_0) = \Phi_N(\Phi_{N-1}(\cdots(\Phi_1(\theta_1; u_0)\cdots,\theta;u_0),\theta;u_0),\] <p>Now using the notation that $u_i$ is the output of the $i$th layer of the network we can write the propagation through the network as a large nonlinear system of equations that have to be satisfied</p> \[f(u,\theta) = u - \Phi(u,\theta) = \underbrace{\begin{bmatrix} u_1 \\ u_2 \\ \vdots \\ u_N \end{bmatrix}}_{u} - \begin{bmatrix} \Phi_1(\theta; u_0) \\ \Phi_2(u_1, \theta; u_0) \\ \vdots \\ \Phi_N(u_1,\ldots,u_{N-1}, \theta; u_0) \end{bmatrix} = \begin{bmatrix}0 \\ 0 \\ \vdots \\ 0\end{bmatrix}.\] <p>Using this notation (and removing the explicit dependence on $x_0$) we see that optimizing a neural network is really nothing more solving the following equality constrained optimization problem</p> \[\begin{aligned} \min_{\theta} \quad &amp; L(u,\theta) = \| u_N - y \|_2^2 + \mu \mathcal{R}(\theta), \\ \text{subject to }\quad &amp; f(u,\theta) = 0. \\ \end{aligned}\] <p>We can now use the adjoint method to compute the gradient of $L$ with respect to $\theta$. As a reminder this means that</p> \[\frac{\mathrm{d}L}{\mathrm{d}\theta} = \frac{\partial L}{\partial\theta} \underbrace{- \frac{\partial L}{\partial u}\left(\frac{\partial f}{\partial u}\right)^{-1}}_{\lambda^\top}\frac{\partial f}{\partial\theta},\] <p>where $\lambda$ is referred to as the adjoint variable. For simplicity, we assume that $u_N$ is just a scalar, that is $u_N = e_N^\top u$ where $e_N$ is the $N$’th canonical basis vector. In this case we have that</p> \[\frac{\partial L}{\partial u} = 2(e_N^\top u - y)e_N^\top = 2(u_N - y)e_N^\top = g_N^\top,\] <p>Now what is left to compute is $\frac{\partial f}{\partial u}$ and $\frac{\partial f}{\partial\theta}$. Starting with $\frac{\partial f}{\partial u}$ we note because $f$ only propagates the $u_i$’s forwards the resulting partial derivative is a lower block triangular matrix of the form</p> \[\frac{\partial f}{\partial u} = \begin{bmatrix} I &amp; 0 &amp; \cdots &amp; 0 \\ \frac{\partial\Phi_2}{\partial u_1} &amp; I &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \frac{\partial\Phi_N}{\partial u_1} &amp; \cdots &amp; \frac{\partial\Phi_N}{\partial u_{N-1}} &amp; I \end{bmatrix} = L,\] <p>Now is a good place to stop and think of a key concept of backpropagation: The resulting computational graph should result in a Directed Acyclic Graph (DAG). This is indeed equivalent to stating that $ \frac{\partial f}{\partial u}$ should be a lower triangular matrix. This is an important property as the adjoint method requires us to invert $ \frac{\partial f}{\partial u} $, which can be done cheaply for a lower triangular matrix. Even more importantly is that the diagonal blocks are the identity, meaning that forward/backward substitutions can be done without any matrix inversions at all.</p> <p>What is left is to compute $\frac{\partial f}{\partial \theta}$, which standardly is done as</p> \[\frac{\partial f}{\partial \theta} = -\underbrace{\begin{bmatrix} \frac{\partial\Phi_1}{\partial \theta_1} &amp; \cdots &amp; \frac{\partial\Phi_1}{\partial \theta_k} \\ \vdots &amp; \ddots &amp; \vdots \\ \frac{\partial\Phi_N}{\partial \theta_1} &amp; \cdots &amp; \frac{\partial\Phi_N}{\partial \theta_k} \end{bmatrix}}_{M^\top} = -M^\top.\] <p>Now, in case of the layers not sharing any parameters the matrix $M^\top$ will have a block diagonal structure of the form</p> \[M^\top = \text{blkdiag}\left(\frac{\partial\Phi_1}{\partial\theta_1}, \ldots, \frac{\partial\Phi_N}{\partial\theta_N}\right), \quad \theta = \begin{bmatrix}\theta_1 \\ \vdots \\ \theta_N \end{bmatrix},\] <p>where $\theta_i$ are the parameters of layer $i$.</p> <p>Putting everything together we find that the gradient of $L$ with respect to $\theta$ is given by</p> \[\begin{aligned} \frac{\mathrm{d}\mathcal{L}}{\mathrm{d}\theta} &amp;= \frac{\partial L}{\partial\theta} - \frac{\partial L}{\partial u}\left(\frac{\partial f}{\partial u}\right)^{-1}\frac{\partial f}{\partial\theta}\\ &amp;= \mu\frac{\partial \mathcal{R}}{\partial\theta} - \left( g_N^\top L^{-1}(- M^\top)\right)\\ &amp;= \mu\frac{\partial \mathcal{R}}{\partial\theta} + 2(u_N - y)e_N^\top L^{-1}M^\top. \end{aligned}\] <p>As noted previously both $L^{-1}$ and $M^\top$ are structured, meaning that the above can be computed efficiently.</p> <details><summary>How to compute the elements of $L$ and $M^\top$</summary> <p>For a standard linear layer an activation function $\sigma$ is usually applied element wise. In practice this means that we really should look at the rows of $\Phi_i$ separately. For row $j$th the gradients are easily seen to be</p> \[\begin{alignat*}{2} \nabla_{u_{i-1}}\sigma(w_j^\top u_{i-1} + b_j) &amp;= \sigma'(w_j^\top u_{i-1} + b_j)w_j,\quad\ \quad &amp;&amp;\text{Goes in to $L$}\\ \nabla_{w_j}\sigma(w_j^\top u_{i-1} + b_j) &amp;= \sigma'(w_j^\top u_{i-1} + b_j)u_{i-1},\quad &amp;&amp;\text{Goes in to $M^\top$}\\ \nabla_{b_j}\sigma(w_j^\top u_{i-1} + b_j) &amp;= \sigma'(w_j^\top u_{i-1} + b_j), \quad\ \quad &amp;&amp;\text{Goes in to $M^\top$} \end{alignat*}\] <p>Using the above we see that</p> \[\frac{\partial\Phi_i}{\partial u_{i-1}} = \text{diag}(\sigma'(W_i u_{i-1} + b))W_i,\] <p>where $\sigma’(\cdot)$ is applied element wise. Now for $\frac{\partial\Phi_i}{\partial\theta_i}$ we have to pick a way of how to vectorize the parameters. Here we choose</p> \[\theta_i = \begin{bmatrix} \text{vec}(W_i) \\ b_i\end{bmatrix},\] <p>where $\text{vec}(W)$ vectorizes by stacking the columns of $W$. Using the vectorization we can write the sought after sensitivity as</p> \[\frac{\partial\Phi_i}{\partial\theta_i} = \text{diag}(\sigma'(W_i u_{i-1} + b))\left( \begin{bmatrix}u_{i-1} &amp; 1\end{bmatrix} \otimes I_{n_i}\right).\] <p>While the Kronecker above does result in a sparse matrix, it is even more structured. Using the standard property of Kronecker products, i.e. $(A\otimes B )v = B \text{mat}(v) A^\top $, where $\text{mat}(v)$ is the inverse operation of $\text{vec}(W)$.-</p> <p>A final remark here is that the diagonal matrices that goes into the elements of $L$ and $M^\top$ are the same. That is if we define</p> \[\begin{aligned} D &amp;= \text{diag}(\sigma'(W_1 u_{0} + b_1), \dots, \sigma'(W_N u_{N-1} + b_N)) \\ &amp;= \text{blkdiag}(\text{diag}(\sigma'(W_1 u_{0} + b_1)), \dots, \text{diag}(\sigma'(W_N u_{N-1} + b_N))), \end{aligned}\] <p>then it follows that</p> \[\begin{aligned} L &amp;= I - D\text{blkdiag}(W_1, \dots, W_{N-1},-1), \quad \\ M^\top &amp;= D\text{blkdiag}\left(\begin{bmatrix}u_0 &amp; 1\end{bmatrix} \otimes I_{n_1}, \dots, \begin{bmatrix}u_{N-1} &amp; 1\end{bmatrix} \otimes I_{n_N}\right). \end{aligned}\] <p>with the little unusual notation of $\text{blkdiag}(\cdot,-1)$ means that is the lower block diagonal matrix.</p> </details> <details><summary>Julia implementation</summary> <div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># add https://github.com/mipals/BlockDiagonalMatrices.jl.git # For BlockDiagonalMatrices</span>
<span class="k">using</span> <span class="n">Test</span><span class="x">,</span> <span class="n">ForwardDiff</span><span class="x">,</span> <span class="n">LinearAlgebra</span><span class="x">,</span> <span class="n">BlockBandedMatrices</span><span class="x">,</span> <span class="n">SparseArrays</span><span class="x">,</span> <span class="n">BlockDiagonalMatrices</span><span class="x">,</span> <span class="n">Kronecker</span>
<span class="n">h</span><span class="x">(</span><span class="n">x</span><span class="x">)</span>  <span class="o">=</span>  <span class="n">exp</span><span class="x">(</span><span class="o">-</span><span class="n">x</span><span class="x">)</span> <span class="c"># activation function</span>
<span class="n">∇h</span><span class="x">(</span><span class="n">x</span><span class="x">)</span> <span class="o">=</span> <span class="o">-</span><span class="n">exp</span><span class="x">(</span><span class="o">-</span><span class="n">x</span><span class="x">)</span> <span class="c"># derivative of activation function</span>

<span class="c"># Forward pass including the derivatives</span>
<span class="k">function</span><span class="nf"> forward_pass</span><span class="x">(</span><span class="n">u0</span><span class="x">,</span><span class="n">θ</span><span class="x">)</span>
    <span class="n">x0</span> <span class="o">=</span> <span class="n">u0</span>
    <span class="n">diags</span> <span class="o">=</span> <span class="n">empty</span><span class="x">([</span><span class="n">first</span><span class="x">(</span><span class="n">θ</span><span class="x">)[</span><span class="mi">2</span><span class="x">]])</span>
    <span class="n">krons</span> <span class="o">=</span> <span class="n">empty</span><span class="x">([</span><span class="n">first</span><span class="x">(</span><span class="n">θ</span><span class="x">)[</span><span class="mi">2</span><span class="x">]</span><span class="err">'</span> <span class="n">⊗</span> <span class="n">I</span><span class="x">(</span><span class="mi">2</span><span class="x">)</span> <span class="x">])</span>
    <span class="k">for</span> <span class="x">(</span><span class="n">W</span><span class="x">,</span><span class="n">b</span><span class="x">)</span> <span class="k">in</span> <span class="n">θ</span>
        <span class="n">push!</span><span class="x">(</span><span class="n">krons</span><span class="x">,[</span><span class="n">x0</span><span class="x">;</span> <span class="mi">1</span><span class="x">]</span><span class="err">'</span> <span class="n">⊗</span> <span class="n">I</span><span class="x">(</span><span class="n">length</span><span class="x">(</span><span class="n">b</span><span class="x">)))</span> <span class="c"># Lazy Kronecker producect using Kronecker.jl</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">W</span><span class="o">*</span><span class="n">x0</span> <span class="o">+</span> <span class="n">b</span>  <span class="c"># Can be used for both forward pass and derivative pass</span>
        <span class="n">x0</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="x">(</span><span class="n">tmp</span><span class="x">)</span>
        <span class="n">push!</span><span class="x">(</span><span class="n">diags</span><span class="x">,</span> <span class="n">∇h</span><span class="o">.</span><span class="x">(</span><span class="n">tmp</span><span class="x">))</span>
    <span class="k">end</span>
    <span class="k">return</span> <span class="n">krons</span><span class="x">,</span> <span class="n">diags</span><span class="x">,</span> <span class="n">x0</span>
<span class="k">end</span>
<span class="c"># Block backsubstitution: Solving L^(-T)y = b</span>
<span class="k">function</span><span class="nf"> backsub</span><span class="x">(</span><span class="n">dblks</span><span class="x">,</span><span class="n">wblks</span><span class="x">,</span><span class="n">b</span><span class="x">)</span>
    <span class="n">y</span>  <span class="o">=</span> <span class="n">convert</span><span class="o">.</span><span class="x">(</span><span class="n">eltype</span><span class="x">(</span><span class="n">wblks</span><span class="x">[</span><span class="mi">1</span><span class="x">]),</span> <span class="n">copy</span><span class="x">(</span><span class="n">b</span><span class="x">))</span>
    <span class="n">j0</span> <span class="o">=</span> <span class="n">length</span><span class="x">(</span><span class="n">b</span><span class="x">)</span>
    <span class="n">i0</span> <span class="o">=</span> <span class="n">length</span><span class="x">(</span><span class="n">b</span><span class="x">)</span> <span class="o">-</span> <span class="n">size</span><span class="x">(</span><span class="n">wblks</span><span class="x">[</span><span class="k">end</span><span class="x">],</span><span class="mi">1</span><span class="x">)</span>
    <span class="nd">@views</span> <span class="k">for</span> <span class="x">(</span><span class="n">D</span><span class="x">,</span><span class="n">blk</span><span class="x">)</span> <span class="k">in</span> <span class="x">(</span><span class="n">zip</span><span class="x">(</span><span class="n">reverse</span><span class="x">(</span><span class="n">dblks</span><span class="x">),</span><span class="n">reverse</span><span class="x">(</span><span class="kt">Transpose</span><span class="o">.</span><span class="x">(</span><span class="n">wblks</span><span class="x">))))</span>
        <span class="n">i1</span><span class="x">,</span><span class="n">j1</span> <span class="o">=</span> <span class="n">size</span><span class="x">(</span><span class="n">blk</span><span class="x">)</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">D</span> <span class="o">.*</span> <span class="n">y</span><span class="x">[</span><span class="n">j0</span><span class="o">-</span><span class="n">j1</span><span class="o">+</span><span class="mi">1</span><span class="o">:</span><span class="n">j0</span><span class="x">]</span>
        <span class="n">mul!</span><span class="x">(</span><span class="n">y</span><span class="x">[</span><span class="n">i0</span><span class="o">-</span><span class="n">i1</span><span class="o">+</span><span class="mi">1</span><span class="o">:</span><span class="n">i0</span><span class="x">],</span> <span class="n">blk</span><span class="x">,</span> <span class="n">tmp</span><span class="x">,</span> <span class="mi">1</span><span class="x">,</span> <span class="mi">1</span><span class="x">)</span>
        <span class="n">j0</span> <span class="o">-=</span> <span class="n">j1</span>
        <span class="n">i0</span> <span class="o">-=</span> <span class="n">i1</span>
    <span class="k">end</span>
    <span class="k">return</span> <span class="n">y</span>
<span class="k">end</span>
<span class="c"># Helper function thats packs a vector θ into Ws and bs</span>
<span class="k">function</span><span class="nf"> pack_θ</span><span class="x">(</span><span class="n">θ</span><span class="x">,</span> <span class="n">Ws_sizes</span><span class="x">,</span> <span class="n">bs_sizes</span><span class="x">)</span>
    <span class="n">We</span> <span class="o">=</span> <span class="n">empty</span><span class="x">([</span><span class="n">ones</span><span class="x">(</span><span class="n">eltype</span><span class="x">(</span><span class="n">θ</span><span class="x">),</span><span class="mi">1</span><span class="x">,</span><span class="mi">1</span><span class="x">)])</span>
    <span class="n">be</span> <span class="o">=</span> <span class="n">empty</span><span class="x">([</span><span class="n">ones</span><span class="x">(</span><span class="n">eltype</span><span class="x">(</span><span class="n">θ</span><span class="x">),</span><span class="mi">1</span><span class="x">)])</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="x">(</span><span class="n">W_size</span><span class="x">,</span><span class="n">b_size</span><span class="x">)</span> <span class="k">in</span> <span class="n">zip</span><span class="x">(</span><span class="n">Ws_sizes</span><span class="x">,</span> <span class="n">bs_sizes</span><span class="x">)</span>
        <span class="n">j</span> <span class="o">=</span> <span class="n">i</span><span class="o">+</span><span class="n">prod</span><span class="x">(</span><span class="n">W_size</span><span class="x">)</span>
        <span class="n">push!</span><span class="x">(</span><span class="n">We</span><span class="x">,</span> <span class="n">reshape</span><span class="x">(</span><span class="n">θ</span><span class="x">[</span><span class="n">i</span><span class="o">:</span><span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="x">],</span> <span class="n">W_size</span><span class="o">...</span><span class="x">))</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">j</span> <span class="o">+</span> <span class="n">b_size</span>
        <span class="n">push!</span><span class="x">(</span><span class="n">be</span><span class="x">,</span> <span class="n">θ</span><span class="x">[</span><span class="n">j</span><span class="o">:</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="x">])</span>
    <span class="k">end</span>
    <span class="k">return</span> <span class="n">We</span><span class="x">,</span> <span class="n">be</span>
<span class="k">end</span>
<span class="c"># Evaluating f using the forward pass</span>
<span class="k">function</span><span class="nf"> eval_f</span><span class="x">(</span><span class="n">θ</span><span class="x">,</span> <span class="n">Ws_sizes</span><span class="x">,</span> <span class="n">bs_sizes</span><span class="x">,</span> <span class="n">u0</span><span class="x">)</span>
    <span class="n">We</span><span class="x">,</span><span class="n">be</span> <span class="o">=</span> <span class="n">pack_θ</span><span class="x">(</span><span class="n">θ</span><span class="x">,</span> <span class="n">Ws_sizes</span><span class="x">,</span> <span class="n">bs_sizes</span><span class="x">)</span>
    <span class="n">_</span><span class="x">,</span><span class="n">_</span><span class="x">,</span><span class="n">uN</span> <span class="o">=</span> <span class="n">forward_pass</span><span class="x">(</span><span class="n">u0</span><span class="x">,</span> <span class="n">zip</span><span class="x">(</span><span class="n">We</span><span class="x">,</span><span class="n">be</span><span class="x">))</span>
    <span class="k">return</span> <span class="n">uN</span>
<span class="k">end</span>
<span class="c"># Gradient computation using the adjoint method</span>
<span class="k">function</span><span class="nf"> ∇f</span><span class="x">(</span><span class="n">θ</span><span class="x">,</span> <span class="n">Ws_sizes</span><span class="x">,</span> <span class="n">bs_sizes</span><span class="x">,</span> <span class="n">u0</span><span class="x">;</span> <span class="n">y</span><span class="o">=</span><span class="mf">0.0</span><span class="x">)</span>
    <span class="c"># First pack vector parameters to matrices</span>
    <span class="n">We</span><span class="x">,</span><span class="n">be</span> <span class="o">=</span> <span class="n">pack_θ</span><span class="x">(</span><span class="n">θ</span><span class="x">,</span> <span class="n">Ws_sizes</span><span class="x">,</span> <span class="n">bs_sizes</span><span class="x">)</span>
    <span class="c"># Forward parse includes derivative information</span>
    <span class="n">krons</span><span class="x">,</span> <span class="n">ddiags</span><span class="x">,</span> <span class="n">uN</span> <span class="o">=</span> <span class="n">forward_pass</span><span class="x">(</span><span class="n">u0</span><span class="x">,</span> <span class="n">zip</span><span class="x">(</span><span class="n">We</span><span class="x">,</span><span class="n">be</span><span class="x">))</span>
    <span class="c"># We here use that L' = I - blkdiag(W_1,..., W_N)D' and M^T = D*K </span>
    <span class="n">D</span> <span class="o">=</span> <span class="kt">Diagonal</span><span class="x">(</span><span class="n">vcat</span><span class="x">(</span><span class="n">ddiags</span><span class="o">...</span><span class="x">))</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">BlockDiagonal</span><span class="x">(</span><span class="n">krons</span><span class="x">)</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">zeros</span><span class="x">(</span><span class="n">eltype</span><span class="x">(</span><span class="n">θ</span><span class="x">),</span> <span class="n">sum</span><span class="x">(</span><span class="n">w_sizes</span> <span class="o">-&gt;</span> <span class="n">w_sizes</span><span class="x">[</span><span class="mi">1</span><span class="x">],</span> <span class="n">Ws_sizes</span><span class="x">))</span>
    <span class="n">g</span><span class="x">[</span><span class="k">end</span><span class="x">]</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="x">(</span><span class="n">uN</span><span class="x">[</span><span class="mi">1</span><span class="x">]</span> <span class="o">-</span> <span class="n">y</span><span class="x">)</span> <span class="c"># uN is a 1x1 matrix so extract the scalar</span>
    <span class="c"># The final step is to evaluate the gradient from the right</span>
    <span class="n">grad_adjoint</span> <span class="o">=</span> <span class="x">(</span><span class="n">backsub</span><span class="x">(</span><span class="n">ddiags</span><span class="x">,</span><span class="n">We</span><span class="x">[</span><span class="mi">2</span><span class="o">:</span><span class="k">end</span><span class="x">],</span><span class="n">g</span><span class="err">'</span><span class="x">)</span><span class="o">*</span><span class="n">D</span><span class="x">)</span><span class="o">*</span><span class="n">K</span>
    <span class="k">return</span> <span class="n">grad_adjoint</span><span class="err">'</span>
<span class="k">end</span>
<span class="c"># Forward difference to test the adjoint gradient implementation</span>
<span class="k">function</span><span class="nf"> fd</span><span class="x">(</span><span class="n">θ</span><span class="x">,</span> <span class="n">Ws_sizes</span><span class="x">,</span><span class="n">bs_sizes</span><span class="x">,</span><span class="n">u0</span><span class="x">,</span><span class="n">i</span><span class="x">;</span><span class="n">e</span><span class="o">=</span><span class="mf">1e-6</span><span class="x">,</span><span class="n">y</span><span class="o">=</span><span class="mf">2.0</span><span class="x">)</span>
    <span class="n">f0</span> <span class="o">=</span> <span class="n">eval_f</span><span class="x">(</span><span class="n">θ</span><span class="x">,</span> <span class="n">Ws_sizes</span><span class="x">,</span> <span class="n">bs_sizes</span><span class="x">,</span> <span class="n">u0</span><span class="x">)</span>
    <span class="n">θ</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">+=</span> <span class="n">e</span>
    <span class="n">f1</span> <span class="o">=</span> <span class="n">eval_f</span><span class="x">(</span><span class="n">θ</span><span class="x">,</span> <span class="n">Ws_sizes</span><span class="x">,</span> <span class="n">bs_sizes</span><span class="x">,</span> <span class="n">u0</span><span class="x">)</span>
    <span class="n">θ</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">-=</span> <span class="n">e</span>
    <span class="k">return</span> <span class="n">sum</span><span class="x">(((</span><span class="n">f1</span><span class="x">[</span><span class="mi">1</span><span class="x">]</span> <span class="o">-</span> <span class="n">y</span><span class="x">)</span><span class="o">^</span><span class="mi">2</span> <span class="o">-</span> <span class="x">(</span><span class="n">f0</span><span class="x">[</span><span class="mi">1</span><span class="x">]</span> <span class="o">-</span> <span class="n">y</span><span class="x">)</span><span class="o">^</span><span class="mi">2</span><span class="x">)</span><span class="o">/</span><span class="n">e</span><span class="x">)</span>
<span class="k">end</span>

<span class="c"># Setting up parameters </span>
<span class="n">layer_sizes</span> <span class="o">=</span> <span class="x">[</span><span class="mi">50</span><span class="x">,</span><span class="mi">40</span><span class="x">,</span><span class="mi">30</span><span class="x">,</span><span class="mi">20</span><span class="x">,</span><span class="mi">10</span><span class="x">,</span><span class="mi">1</span><span class="x">]</span>
<span class="n">N</span> <span class="o">=</span> <span class="n">length</span><span class="x">(</span><span class="n">layer_sizes</span><span class="x">)</span> <span class="o">-</span> <span class="mi">1</span>
<span class="n">init</span><span class="x">(</span><span class="n">sizes</span><span class="o">...</span><span class="x">)</span> <span class="o">=</span> <span class="mf">0.01</span><span class="o">*</span><span class="n">randn</span><span class="x">(</span><span class="n">sizes</span><span class="o">...</span><span class="x">)</span>
<span class="n">Ws</span> <span class="o">=</span> <span class="x">[</span><span class="n">init</span><span class="x">(</span><span class="n">layer_sizes</span><span class="x">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="x">],</span><span class="n">layer_sizes</span><span class="x">[</span><span class="n">i</span><span class="x">])</span> <span class="k">for</span> <span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="o">:</span><span class="n">N</span><span class="x">]</span>
<span class="n">bs</span> <span class="o">=</span> <span class="x">[</span><span class="n">init</span><span class="x">(</span><span class="n">layer_sizes</span><span class="x">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="x">])</span> <span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="o">:</span><span class="n">N</span><span class="x">]</span>
<span class="n">u0</span> <span class="o">=</span> <span class="n">init</span><span class="x">(</span><span class="n">layer_sizes</span><span class="x">[</span><span class="mi">1</span><span class="x">],</span><span class="mi">1</span><span class="x">)[</span><span class="o">:</span><span class="x">]</span>
<span class="n">θ</span>  <span class="o">=</span> <span class="n">zip</span><span class="x">(</span><span class="n">Ws</span><span class="x">,</span><span class="n">bs</span><span class="x">)</span>

<span class="n">Ws_sizes</span> <span class="o">=</span> <span class="n">size</span><span class="o">.</span><span class="x">(</span><span class="n">Ws</span><span class="x">)</span>
<span class="n">bs_sizes</span> <span class="o">=</span> <span class="n">length</span><span class="o">.</span><span class="x">(</span><span class="n">bs</span><span class="x">)</span>

<span class="c"># First we compute the forward pass</span>
<span class="n">θvec</span> <span class="o">=</span> <span class="n">vcat</span><span class="x">([[</span><span class="n">W</span><span class="x">[</span><span class="o">:</span><span class="x">];</span> <span class="n">b</span><span class="x">]</span> <span class="k">for</span> <span class="x">(</span><span class="n">W</span><span class="x">,</span><span class="n">b</span><span class="x">)</span> <span class="k">in</span> <span class="n">θ</span><span class="x">]</span><span class="o">...</span><span class="x">)</span>

<span class="c">## Testing the gradient</span>
<span class="n">y</span> <span class="o">=</span> <span class="mf">3.0</span> <span class="c"># We aim to have the final output be 3.0</span>
<span class="n">grad_adjoint</span> <span class="o">=</span> <span class="n">∇f</span><span class="x">(</span><span class="n">θvec</span><span class="x">,</span> <span class="n">Ws_sizes</span><span class="x">,</span> <span class="n">bs_sizes</span><span class="x">,</span> <span class="n">u0</span><span class="x">;</span><span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="x">)</span>
<span class="n">idx</span> <span class="o">=</span> <span class="mi">4000</span>
<span class="nd">@test</span> <span class="n">fd</span><span class="x">(</span><span class="n">θvec</span><span class="x">,</span> <span class="n">Ws_sizes</span><span class="x">,</span><span class="n">bs_sizes</span><span class="x">,</span><span class="n">u0</span><span class="x">,</span><span class="n">idx</span><span class="x">;</span><span class="n">e</span><span class="o">=</span><span class="mf">1e-5</span><span class="x">,</span><span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="x">)</span> <span class="n">≈</span> <span class="n">grad_adjoint</span><span class="x">[</span><span class="n">idx</span><span class="x">]</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-6</span>

<span class="c"># Optimizing to get the output y</span>
<span class="k">for</span> <span class="n">iter</span> <span class="o">=</span> <span class="mi">1</span><span class="o">:</span><span class="mi">1000</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">∇f</span><span class="x">(</span><span class="n">θvec</span><span class="x">,</span> <span class="n">Ws_sizes</span><span class="x">,</span> <span class="n">bs_sizes</span><span class="x">,</span> <span class="n">u0</span><span class="x">;</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="x">)</span> <span class="c"># Y is the output value we want</span>
    <span class="n">θvec</span> <span class="o">-=</span> <span class="mf">0.001</span><span class="o">*</span><span class="n">grad</span>
<span class="k">end</span>
<span class="nd">@test</span> <span class="n">eval_f</span><span class="x">(</span><span class="n">θvec</span><span class="x">,</span><span class="n">Ws_sizes</span><span class="x">,</span><span class="n">bs_sizes</span><span class="x">,</span><span class="n">u0</span><span class="x">)[</span><span class="mi">1</span><span class="x">]</span> <span class="n">≈</span> <span class="n">y</span>
<span class="nd">@test</span> <span class="n">∇f</span><span class="x">(</span><span class="n">θvec</span><span class="x">,</span> <span class="n">Ws_sizes</span><span class="x">,</span> <span class="n">bs_sizes</span><span class="x">,</span> <span class="n">u0</span><span class="x">;</span><span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="x">)</span> <span class="n">≈</span> <span class="n">zeros</span><span class="x">(</span><span class="n">length</span><span class="x">(</span><span class="n">θvec</span><span class="x">))</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-10</span>

</code></pre></div></div> </details> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib"></d-bibliography> <d-article> <br> <br> <div id="giscus_thread"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'mipals/mipals.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Mikkel Paltorp. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?5a16f3fff810d102bb6b8d68a4e2e358"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>