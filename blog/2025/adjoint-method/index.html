<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Adjoint Method | Mikkel Paltorp </title> <meta name="author" content="Mikkel Paltorp"> <meta name="description" content="and why its easier than people make it out to be"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%94%AC&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://mipals.github.io/blog/2025/adjoint-method/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "The Adjoint Method",
            "description": "and why its easier than people make it out to be",
            "published": "August 01, 2025",
            "authors": [
              
              {
                "author": "Mikkel Paltorp",
                "authorURL": "https://mipals.github.io",
                "affiliations": [
                  {
                    "name": "Technical University of Denmark",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Mikkel</span> Paltorp </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>The Adjoint Method</h1> <p>and why its easier than people make it out to be</p> </d-title> <d-byline></d-byline> <d-article> <p>In many engineering applications we are interested in solving equality constrained optimization problems where the optimization variable ($\theta$) implicitly defines another variable ($u$) that then appears in the objective function<d-footnote>A famous example of this is [topology optimization](https://en.wikipedia.org/wiki/Topology_optimization).</d-footnote>. Written out this means solving optimization problems of the following form</p> \[\begin{equation} \begin{aligned} \min_{\theta \in \mathbb{R}^{n_\theta}} \quad &amp; L(u,\theta), \quad\ \quad\ \left(\text{some objective function}\right)\\ \text{subject to }\quad &amp; f(u,\theta) = 0, \quad \left(\text{some equality constraint}\right) \\ &amp; u \in \mathbb{R}^{n_u}. \end{aligned} \end{equation}\] <p>Most methods for solving equality constrained optimization problems requires the computation of gradient of the objective function with respect to the optimization variable $\theta$. This is where the problem comes in: Naively computing the gradient requires the computation of the sensitivities of the implicitly defined variable $u$ with respect to the optimization variable $\theta$ $\left(\text{i.e.}\ \frac{\mathrm{d}u}{\mathrm{d}\theta} \in \mathbb{R}^{n_u \times n_\theta}\right)$. This result in a computational bottleneck as forming the sensitivities scales as $\mathcal{O}(n_un_\theta)$, meaning that adding a new parameter adds $n_u$ additional sensitivities (and one adding one more variable would add $n_\theta$ additional sensitivities).</p> <p>To resolve this computational bottleneck we can make use the adjoint method. The first step in the derivation of the adjoint method is to introduce the Lagrangian of the objective function, i.e.</p> \[\begin{equation} \mathcal{L}(u,\theta,\lambda) = L(u,\theta) + \lambda^\top f(u,\theta). \end{equation}\] <p>It is easy to see that whenever $u$ satisfy the equality constraint $f(u,\theta)=0$, then the Lagrangian is equal to the original objective function. This in turn mean that the two gradients $\nabla_\theta L(u,\theta,\lambda)$ and $\nabla_\theta \mathcal{L}(u,\theta,\lambda)$ are equal whenever $u$ satisfy the equality constraint $f(u,\theta) = 0$. The reason why the introduction of the additional term in the Lagrangian is useful is that $\lambda$ can be set to be anything. In particular, we aim to set in a way so that we can avoid computing the otherwise computational expensive sensitivities $\frac{\mathrm{d}u}{\mathrm{d}\theta}$. We start by computing the total derivative of the Lagrangian with respect to $\theta$<d-footnote>The total derivative is the transpose of the gradient i.e. $\left(\frac{\mathrm{d}L}{\mathrm{d}\theta}\right)^\top = \nabla_\theta L$. That is multiplying the change in $\theta$ with the total derivative (rather than the transpose of the gradient) gives the change in the function $L$.</d-footnote></p> \[\begin{equation} \frac{\mathrm{d}\mathcal{L}}{\mathrm{d}\theta} = \frac{\partial L}{\partial\theta} + \frac{\partial L}{\partial u}\frac{\mathrm{d}u}{\mathrm{d}\theta} + \lambda^\top\left(\frac{\partial f}{\partial\theta} + \frac{\partial f}{\partial u}\frac{\mathrm{d}u}{\mathrm{d}\theta}\right). \end{equation}\] <p>Now we collect the terms that depend on the undesired $\frac{\mathrm{d}u}{\mathrm{d}\theta}$ result in</p> \[\begin{equation} \frac{\mathrm{d}\mathcal{L}}{\mathrm{d}\theta} = \frac{\partial L}{\partial\theta} + \lambda^\top\frac{\partial f}{\partial\theta} + \left(\frac{\partial L}{\partial u} + \lambda^\top\frac{\partial f}{\partial u}\right)\frac{\mathrm{d}u}{\mathrm{d}\theta} . \end{equation}\] <p>As we can choose $\lambda$ freely a natural idea is to set it in a way such that the term in front of the undesired term vanishes. This means that we can choose $\lambda$ as the solution to the equation</p> \[\begin{equation} \frac{\partial L}{\partial u} + \lambda^\top\frac{\partial f}{\partial u} = 0 \quad \Rightarrow \quad \lambda^\top = -\frac{\partial L}{\partial u}\left(\frac{\partial f}{\partial u}\right)^{-1}. \end{equation}\] <p>Inserting $\lambda^\top$ back into the equation we find that the gradient of the Lagrangian with respect to $\theta$ is given by</p> \[\begin{equation} \frac{\mathrm{d}\mathcal{L}}{\mathrm{d}\theta} = \frac{\partial L}{\partial\theta} \underbrace{- \frac{\partial L}{\partial u}\left(\frac{\partial f}{\partial u}\right)^{-1}}_{\lambda^\top}\frac{\partial f}{\partial\theta} \left(= \frac{\mathrm{d}L}{\mathrm{d}\theta}\right). \end{equation}\] <p>To conclude: The adjoint method is a simple way to avoid the computational bottleneck of computing the sensitivities $\frac{\mathrm{d}u}{\mathrm{d}\theta}$ by cleverly computing the so-called adjoint variable $\lambda$ in a way that eliminates the need to compute the problematic sensitivities.</p> <h3 id="example">Example</h3> <p>In order to illustrate the adjoint method we consider a simple linearly constrained problem of the form (inspiration from problem 38 in <d-cite key="bright2025matrixcalculusformachine"></d-cite>)</p> \[\begin{aligned} L(u,\theta) &amp;= \left(c^\top u(\theta)\right)^2, \\ f(u,\theta) &amp;= A(\theta)u - b = 0, \end{aligned}\] <p>where $A(\theta) \in \mathbb{R}^{n\times n}$ is a symmetric tridiagonal matrix that depends on the parameters $\theta \in \mathbb{R}^{2n-1}$ as</p> \[A = \begin{bmatrix} \theta_1 &amp; \theta_{n+1} &amp; &amp; &amp; 0 \\ \theta_{n+1} &amp; \theta_2 &amp; \ddots &amp; &amp; \\ &amp; \ddots &amp; \ddots &amp; \ddots &amp; \\ &amp; &amp; \ddots &amp; \theta_{n-1} &amp; \theta_{2n-1} \\ 0 &amp; &amp; &amp; \theta_{2n-1} &amp; \theta_n \end{bmatrix}.\] <p>Now in order to compute the gradient of interested we start by computing the adjoint variable $\lambda$ as</p> \[\lambda^\top = -\frac{\partial L}{\partial u}\left(\frac{\partial f}{\partial u}\right)^{-1} = -2\left(c^\top u\right)c^\top A(\theta)^{-1}.\] <p>Note that in practice we do not form $A(\theta)^{-1}$ explicitly but rather compute $\lambda$ by solving the linear system $ A(\theta)^\top\lambda = -2c\left(c^\top u\right)$. Using the adjoint variable we can compute the gradient of $L$ with respect to $\theta$ as</p> \[\frac{\mathrm{d}L}{\mathrm{d}\theta} = \underbrace{\frac{\partial L}{\partial\theta}}_{=0} + \lambda^\top\frac{\partial f}{\partial\theta} = \lambda^\top\frac{\partial A}{\partial\theta}u.\] <p>In the concrete case of the derivatives of $A$ with respect to $\theta_i$ we have that</p> \[\frac{\partial A}{\partial \theta_i} = \begin{bmatrix} \delta_{i,1} &amp; \delta_{i,n+1} &amp; &amp; &amp; 0 \\ \delta_{i,n+1} &amp; \delta_{i,2} &amp; \ddots &amp; &amp; \\ &amp; \ddots &amp; \ddots &amp; \ddots &amp; \\ &amp; &amp; \ddots &amp; \delta_{i,n-1} &amp; \delta_{i,2n-1} \\ 0 &amp; &amp; &amp; \delta_{i,2n-1} &amp; \delta_{i,n} \end{bmatrix}.\] <p>Using this it follows that the $i$th component of the gradient can be computed as</p> \[\lambda^\top\frac{\partial A}{\partial \theta_i}u = \begin{cases} \lambda_i u_i, \quad &amp;i \leq n, \\ \lambda_{i+1-n}u_{i-n} + \lambda_{i-n} u_{i+1-n}, \quad &amp;i &gt; n. \end{cases}\] <details><summary>Code</summary> <div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">LinearAlgebra</span> 
<span class="n">n</span> <span class="o">=</span> <span class="mi">1000</span>        <span class="c"># Number of linear constraints</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">rand</span><span class="x">(</span><span class="n">n</span><span class="x">)</span>     <span class="c"># Objective vector</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">rand</span><span class="x">(</span><span class="n">n</span><span class="x">)</span>     <span class="c"># Linear equality vector</span>
<span class="n">θ</span> <span class="o">=</span> <span class="n">rand</span><span class="x">(</span><span class="mi">2</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="x">)</span>  <span class="c"># Parameters of the equality constraint</span>

<span class="c"># Parametrizing linear equality constraint matrix</span>
<span class="n">A</span><span class="x">(</span><span class="n">θ</span><span class="x">)</span> <span class="o">=</span> <span class="kt">SymTridiagonal</span><span class="x">(</span><span class="n">θ</span><span class="x">[</span><span class="mi">1</span><span class="o">:</span><span class="n">n</span><span class="x">],</span> <span class="n">θ</span><span class="x">[</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="o">:</span><span class="k">end</span><span class="x">])</span> 

<span class="c"># Objective function</span>
<span class="n">f</span><span class="x">(</span><span class="n">θ</span><span class="x">)</span> <span class="o">=</span> <span class="x">(</span><span class="n">c</span><span class="err">'</span> <span class="o">*</span> <span class="x">(</span><span class="n">A</span><span class="x">(</span><span class="n">θ</span><span class="x">)</span> <span class="o">\</span> <span class="n">b</span><span class="x">))</span><span class="o">^</span><span class="mi">2</span> 

<span class="c"># Partial derivatives</span>
<span class="n">partial</span><span class="x">(</span><span class="n">i</span><span class="x">,</span><span class="n">n</span><span class="x">,</span><span class="n">λ</span><span class="x">,</span><span class="n">u</span><span class="x">)</span> <span class="o">=</span> <span class="n">i</span> <span class="o">&lt;=</span> <span class="n">n</span> <span class="o">?</span>  <span class="n">λ</span><span class="x">[</span><span class="n">i</span><span class="x">]</span><span class="o">*</span><span class="n">u</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">:</span> <span class="n">λ</span><span class="x">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="o">-</span><span class="n">n</span><span class="x">]</span><span class="o">*</span><span class="n">u</span><span class="x">[</span><span class="n">i</span><span class="o">-</span><span class="n">n</span><span class="x">]</span> <span class="o">+</span> <span class="n">λ</span><span class="x">[</span><span class="n">i</span><span class="o">-</span><span class="n">n</span><span class="x">]</span><span class="o">*</span><span class="n">u</span><span class="x">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="o">-</span><span class="n">n</span><span class="x">]</span>

<span class="c"># Gradient computation</span>
<span class="k">function</span><span class="nf"> ∇f</span><span class="x">(</span><span class="n">θ</span><span class="x">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">length</span><span class="x">(</span><span class="n">b</span><span class="x">)</span>
    <span class="n">M</span> <span class="o">=</span> <span class="n">A</span><span class="x">(</span><span class="n">θ</span><span class="x">)</span>                <span class="c"># Defining constraint matrix</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">M</span> <span class="o">\</span> <span class="n">b</span>               <span class="c"># Computing solution</span>
    <span class="n">λ</span> <span class="o">=</span> <span class="n">M</span> <span class="o">\</span> <span class="x">(</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">c</span><span class="o">*</span><span class="x">(</span><span class="n">c</span><span class="err">'</span><span class="o">*</span><span class="n">u</span><span class="x">))</span>   <span class="c"># Adjoint variables</span>
    <span class="k">return</span> <span class="x">[</span><span class="n">partial</span><span class="x">(</span><span class="n">i</span><span class="x">,</span><span class="n">n</span><span class="x">,</span><span class="n">λ</span><span class="x">,</span><span class="n">u</span><span class="x">)</span> <span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="o">:</span><span class="mi">2</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="x">]</span>
<span class="k">end</span>

<span class="c"># Testing against ForwardDiff</span>
<span class="k">using</span> <span class="n">Test</span><span class="x">,</span> <span class="n">ForwardDiff</span>
<span class="nd">@test</span> <span class="n">ForwardDiff</span><span class="o">.</span><span class="n">gradient</span><span class="x">(</span><span class="n">f</span><span class="x">,</span><span class="n">θ</span><span class="x">)</span> <span class="n">≈</span> <span class="n">∇f</span><span class="x">(</span><span class="n">θ</span><span class="x">)</span>
</code></pre></div></div> </details> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib"></d-bibliography> <d-article> <br> <br> <div id="giscus_thread"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'mipals/mipals.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Mikkel Paltorp. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?5a16f3fff810d102bb6b8d68a4e2e358"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>