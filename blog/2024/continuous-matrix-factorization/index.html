<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Continuous Matrix Factorizations | Mikkel Paltorp </title> <meta name="author" content="Mikkel Paltorp"> <meta name="description" content="and their relation to kernel approximations"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%94%AC&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://mipals.github.io/blog/2024/continuous-matrix-factorization/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Continuous Matrix Factorizations",
            "description": "and their relation to kernel approximations",
            "published": "May 20, 2024",
            "authors": [
              
              {
                "author": "Mikkel Paltorp",
                "authorURL": "https://mipals.github.io",
                "affiliations": [
                  {
                    "name": "Technical University of Denmark",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Mikkel</span> Paltorp </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Continuous Matrix Factorizations</h1> <p>and their relation to kernel approximations</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#the-discrete-cholesky-factorization">The (Discrete) Cholesky Factorization</a> </div> <div> <a href="#kernel-matrices-and-kernel-functions">Kernel Matrices and Kernel Functions</a> </div> <div> <a href="#the-continuous-cholesky-factorization">The Continuous Cholesky Factorization</a> </div> <div> <a href="#visual-representations">Visual Representations</a> </div> <div> <a href="#final-remarks">Final remarks</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>This note was inspired by the plenary talk by Joel A. Tropp at SIAM LA24. While I could not find the specific talk online a very similar talk was recorded at the <a href="https://www.youtube.com/watch?v=A9D8KT6N1-8" rel="external nofollow noopener" target="_blank">Boeing Colloquium Series</a>. In the talk Joel presented the work of <d-cite key="chen2024a"></d-cite> , and I will in this note make the relation between that work and that on continuous matrix factorizations by Alex Townsend <d-cite key="townsend2015a"></d-cite>.</p> <h2 id="the-discrete-cholesky-factorization">The (Discrete) Cholesky Factorization</h2> <p>The Cholesky factorization of a positive semidefinite matrix $\mathbf{K}$ is given by</p> \[\mathbf{K} = \mathbf{L}\mathbf{L}^\top \in \mathbb{R}^{n\times n},\] <p>where $\mathbf{L}$ is a lower triangular matrix. Where $\mathbf{L}$ and $\mathbf{L}^\top$ are the factors of interest. The factorization is useful as it can be used to solve the linear systems $\mathbf{K}\mathbf{s} =\mathbf{t}$ by performing two triangular solves ($\mathbf{L}\mathbf{y} = \mathbf{t}, \mathbf{L}^\top\mathbf{s} = \mathbf{y}$). The Cholesky factorization can be computed by performing $n$ rank-1-updates of the original matrix. The first iterate is</p> \[\mathbf{K}^{(0)} = \begin{bmatrix}k &amp; \mathbf{m}^\top \\ \mathbf{m} &amp; \mathbf{M}\end{bmatrix} = \frac{1}{k}\begin{bmatrix}k \\ \mathbf{m}\end{bmatrix}\begin{bmatrix}k \\ \mathbf{m}\end{bmatrix}^\top + \underbrace{\begin{bmatrix} 0 &amp; 0 \\0 &amp; \mathbf{M} - \mathbf{m}\mathbf{m}^\top /k\end{bmatrix}}_{\text{Residual. Denoted by } \mathbf{K}^{(1)}}.\] <p>Each iteration eliminates a row and a column of the <em>residual</em> $\mathbf{K}^{(i)}$, with the first residual being the matrix itself. Thus after $r$ rank-1-update we have eliminated $r$ rows and columns of $\mathbf{K}$. The rows and columns that gets eliminated at the $i$th iteration is called the $i$th <em>pivot</em>. After $n$ iterations every row and column have been eliminated and we have the full factorization.</p> <p>In this note we will mostly be interested in the <em>partial</em> Cholesky factorization, which corresponds to stopping the Cholesky factorization after $r$ iterations. The partial Cholesky factorization will be a rank $r$ approximation of $\mathbf{K}$. The aim is that if $r \ll n$, then the approximation is a “data efficient” representation of $\mathbf{K}$. There exist various of approaches of how to chose the pivots <d-cite key="townsend2015a"></d-cite></p> <details><summary>Pivoting strategies</summary> <p>Let $\mathcal{I}_i$ be the unpicked columns after $i$ iterations. Then we can pick the next pivots as</p> <ul> <li> <p>Greedy: Pick the Next pivot element on the diagonal \(s_i = \text{argmax}_{k\in \mathcal{I}_i}\ \mathbf{A}^{(i-1)}(k,k)\)</p> </li> <li> <p>Uniform: Pick uniformly \(s_i \sim \text{uniform}\{\mathcal{I}_i\}\)</p> </li> <li> <p>Random pivoting: Pick with probability proportional to the diagonal element \(\mathbb{P}\left\{s_i = j\right\} = \frac{\mathbf{A}^{(i-1)}(j,j)}{\text{tr}\ \mathbf{A}^{(i-1)}}, \quad \forall j = 1,\dots,N\)</p> </li> </ul> <p>Note that the above strategies are all the same family of choosing the pivots w.r.t. the Gibbs distribution (the above can be achieved by $\beta \in \lbrace\infty,0,1\rbrace$)</p> \[\mathbb{P}\left\{s_i = j\right\} = \frac{|\mathbf{A}^{(i-1)}(j,j)|^\beta}{\sum_{k=1}^N |\mathbf{A}^{(i-1)}(k,k)|^\beta}, \quad \forall j = 1,\dots,N\] </details> <p>In <d-cite key="chen2024a"></d-cite> it is argued that the randomized approach works best. But why? An intuitive explanation is that this approach does not depend on the ordering of the data which in some cases are benificial.</p> <h2 id="kernel-matrices-and-kernel-functions">Kernel Matrices and Kernel Functions</h2> <p>The motivation for the note is that of Gaussian process regression. A key concept in Gaussian process regression is that of a kernel matrix. The kernel matrix, $\mathbf{K}$, is a matrix whose elements are generated from a so-called kernel function $k(\mathbf{x},\mathbf{y})$.</p> \[\mathbf{K}_{ij} = k(\mathbf{x}_i, \mathbf{x}_j),\ \text{ where }\ \mathbf{x}_i, \mathbf{x}_j \in X\] <p>where $X = \lbrace\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_n \rbrace$ is a collection of data points. An important property of the kernel function is that it is positive semidefinite</p> \[k(\mathbf{x},\mathbf{x}) \geq 0,\] <p>which has the consequence of the resulting kernel matrix $\mathbf{K}$ is positive semidefinite. A commonly used kernel function is the squared exponential given by</p> \[k(\mathbf{x},\mathbf{x}) = \exp\left(-\frac{\|\mathbf{x}_i - \mathbf{x}_j\|_2^2}{2\ell}\right),\] <p>where $\ell$ is commonly referred to as the length scale.</p> <p>To illustrate the process we simulate data points $\mathbf{x}_i \in [-2,2]$ and plot both the discrete / observed kernel matrix $\mathbf{K}$ and the continuos kernel function. Two things to note: First that the $y-$axes below are flipped to conform to the standard kernel matrix form and secondly that the data is sorted, giving some structure to the kernel matrix.</p> <div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">LinearAlgebra</span><span class="x">,</span> <span class="n">StatsBase</span><span class="x">,</span> <span class="n">Plots</span><span class="x">,</span> <span class="n">Random</span><span class="x">,</span> <span class="n">InvertedIndices</span>
<span class="n">Random</span><span class="o">.</span><span class="n">seed!</span><span class="x">(</span><span class="mi">1234</span><span class="x">)</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c"># Number of data points</span>
<span class="n">k</span> <span class="o">=</span> <span class="mi">5</span>   <span class="c"># Rank of approximation</span>
<span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span> <span class="c"># Dimension of data</span>
<span class="n">l</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="c"># Lengthscale parameter</span>

<span class="n">X</span> <span class="o">=</span> <span class="x">[</span><span class="n">rand</span><span class="x">(</span><span class="n">floor</span><span class="x">(</span><span class="kt">Int</span><span class="x">,</span><span class="n">n</span><span class="o">/</span><span class="mi">3</span><span class="x">),</span><span class="n">dim</span><span class="x">)</span> <span class="o">.-</span> <span class="mi">2</span><span class="x">;</span> <span class="n">rand</span><span class="x">(</span><span class="n">floor</span><span class="x">(</span><span class="kt">Int</span><span class="x">,</span><span class="n">n</span><span class="o">/</span><span class="mi">3</span><span class="x">),</span><span class="n">dim</span><span class="x">);</span> <span class="n">rand</span><span class="x">(</span><span class="n">floor</span><span class="x">(</span><span class="kt">Int</span><span class="x">,</span><span class="n">n</span><span class="o">/</span><span class="mi">3</span><span class="x">),</span><span class="n">dim</span><span class="x">)</span><span class="o">/</span><span class="mi">2</span> <span class="o">.+</span> <span class="mf">1.5</span><span class="x">]</span> <span class="c"># Three groups</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">length</span><span class="x">(</span><span class="n">X</span><span class="x">)</span> <span class="c"># Number of data points</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sort</span><span class="x">(</span><span class="n">X</span><span class="x">[</span><span class="o">:</span><span class="x">])</span> <span class="c"># Soring the data - Not strictly necessary, but makes the discrete form nicer</span>
<span class="n">G</span><span class="x">(</span><span class="n">x</span><span class="x">,</span><span class="n">y</span><span class="x">,</span><span class="n">l</span><span class="o">=</span><span class="n">l</span><span class="x">)</span> <span class="o">=</span> <span class="n">exp</span><span class="x">(</span><span class="o">-</span><span class="n">norm</span><span class="x">(</span><span class="n">x</span><span class="o">-</span><span class="n">y</span><span class="x">)</span><span class="o">^</span><span class="mi">2</span><span class="o">/</span><span class="x">(</span><span class="mi">2</span><span class="o">*</span><span class="n">l</span><span class="x">))</span>

<span class="c"># The actual view is in 2D</span>
<span class="n">o</span> <span class="o">=</span> <span class="n">ones</span><span class="x">(</span><span class="n">n</span><span class="x">)</span>
<span class="n">Xx</span> <span class="o">=</span> <span class="n">kron</span><span class="x">(</span><span class="n">X</span><span class="x">,</span><span class="n">o</span><span class="x">)</span>
<span class="n">Xy</span> <span class="o">=</span> <span class="n">kron</span><span class="x">(</span><span class="n">o</span><span class="x">,</span><span class="n">X</span><span class="x">)</span>
<span class="n">Gk</span> <span class="o">=</span> <span class="x">[</span><span class="n">G</span><span class="x">(</span><span class="n">x</span><span class="x">,</span><span class="n">y</span><span class="x">)</span> <span class="k">for</span> <span class="x">(</span><span class="n">x</span><span class="x">,</span><span class="n">y</span><span class="x">)</span> <span class="k">in</span> <span class="n">zip</span><span class="x">(</span><span class="n">Xx</span><span class="x">,</span><span class="n">Xy</span><span class="x">)]</span>
<span class="n">plot_matrix</span> <span class="o">=</span> <span class="n">heatmap</span><span class="x">(</span><span class="n">sort</span><span class="x">(</span><span class="n">X</span><span class="x">[</span><span class="o">:</span><span class="x">]),</span><span class="n">sort</span><span class="x">(</span><span class="n">X</span><span class="x">[</span><span class="o">:</span><span class="x">]),</span><span class="n">reshape</span><span class="x">(</span><span class="n">Gk</span><span class="x">,</span><span class="n">n</span><span class="x">,</span><span class="n">n</span><span class="x">),</span><span class="n">aspect_ratio</span><span class="o">=:</span><span class="n">equal</span><span class="x">,</span> <span class="n">title</span><span class="o">=</span><span class="s">"Discrete"</span><span class="x">)</span>
<span class="n">scatter!</span><span class="x">(</span><span class="n">plot_matrix</span><span class="x">,</span><span class="n">Xx</span><span class="x">,</span><span class="n">Xy</span><span class="x">,</span> <span class="n">label</span><span class="o">=</span><span class="nb">false</span><span class="x">)</span>
<span class="n">xlabel!</span><span class="x">(</span><span class="n">plot_matrix</span><span class="x">,</span><span class="s">"x"</span><span class="x">);</span> <span class="n">ylabel!</span><span class="x">(</span><span class="n">plot_matrix</span><span class="x">,</span><span class="s">"y"</span><span class="x">);</span> <span class="n">yflip!</span><span class="x">(</span><span class="nb">true</span><span class="x">)</span>

<span class="n">Xc</span> <span class="o">=</span> <span class="n">range</span><span class="x">(</span><span class="o">-</span><span class="mi">2</span><span class="x">,</span><span class="mi">2</span><span class="x">,</span><span class="mi">300</span><span class="x">)</span>
<span class="n">plot_smooth</span> <span class="o">=</span> <span class="n">contour</span><span class="x">(</span><span class="n">Xc</span><span class="x">,</span><span class="n">Xc</span><span class="x">,</span> <span class="x">(</span><span class="n">x</span><span class="x">,</span><span class="n">y</span><span class="x">)</span> <span class="o">-&gt;</span> <span class="n">G</span><span class="x">(</span><span class="n">x</span><span class="x">,</span><span class="n">y</span><span class="x">),</span> <span class="n">fill</span><span class="o">=</span><span class="nb">true</span><span class="x">,</span><span class="n">aspect_ratio</span><span class="o">=:</span><span class="n">equal</span><span class="x">,</span><span class="n">clim</span><span class="o">=</span><span class="x">(</span><span class="mi">0</span><span class="x">,</span><span class="mi">1</span><span class="x">))</span>
<span class="n">xlabel!</span><span class="x">(</span><span class="n">plot_smooth</span><span class="x">,</span><span class="s">"x"</span><span class="x">);</span> <span class="n">ylabel!</span><span class="x">(</span><span class="n">plot_smooth</span><span class="x">,</span><span class="s">"y"</span><span class="x">);</span> <span class="n">yflip!</span><span class="x">(</span><span class="nb">true</span><span class="x">)</span>
<span class="n">scatter!</span><span class="x">(</span><span class="n">plot_smooth</span><span class="x">,</span><span class="n">Xx</span><span class="x">,</span><span class="n">Xy</span><span class="x">,</span> <span class="n">label</span><span class="o">=</span><span class="nb">false</span><span class="x">,</span><span class="n">title</span><span class="o">=</span><span class="s">"Continuous"</span><span class="x">)</span>
<span class="n">scatter!</span><span class="x">(</span><span class="n">plot_smooth</span><span class="x">,</span><span class="n">X</span><span class="x">,</span><span class="n">X</span><span class="x">,</span> <span class="n">label</span><span class="o">=</span><span class="nb">false</span><span class="x">,</span> <span class="n">color</span><span class="o">=:</span><span class="n">black</span><span class="x">)</span>
<span class="n">plot</span><span class="x">(</span><span class="n">plot_matrix</span><span class="x">,</span><span class="n">plot_smooth</span><span class="x">,</span> <span class="n">layour</span><span class="o">=</span><span class="x">(</span><span class="mi">1</span><span class="x">,</span><span class="mi">2</span><span class="x">),</span><span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="x">)</span>
</code></pre></div></div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/initial_scatter-480.webp 480w,/assets/img/continuous/initial_scatter-800.webp 800w,/assets/img/continuous/initial_scatter-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/continuous/initial_scatter.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="the-continuous-cholesky-factorization">The Continuous Cholesky Factorization</h2> <p>Approximating the discrete form with a low-rank matrix is as easy performing a partial Cholesky. However, a different approach is needed in order to approximate the continuous form of the “low-rank” function. In <d-cite key="townsend2015a"></d-cite> continuous analogous to matrix factorization is introduced in the context of so-called <em>cmatrices</em>. These matrices can be represented as function of two continuous variables, i.e. that they are elements of $C([a,b]\times[c,d])$. While the properties is only stated for scalar $x$ and $y$ one can easily extend the ideas for a general bivariate function (such as a kernel function).</p> <p>The continuous Cholesky factorization of rank $r$ of kernel function is given as</p> \[k(\mathbf{x},\mathbf{y}) \approx \sum_{i=1}^{r}\frac{k_i(\mathbf{x},\mathbf{x}_i)k_i(\mathbf{x}_i, \mathbf{y})}{k_i(\mathbf{x}_i, \mathbf{x}_i)} = \sum_{i=1}^{r}\frac{k_i(\mathbf{x},\mathbf{x}_i)}{\sqrt{k_i(\mathbf{x}_i, \mathbf{x}_i)}}\frac{k_i(\mathbf{x}_i, \mathbf{y})}{\sqrt{k_i(\mathbf{x}_i, \mathbf{x}_i)}}\] <p>where $(\mathbf{x}_i,\mathbf{x}_i)$ are the so-called pivot points and</p> \[k_i(\mathbf{x},\mathbf{y}) = \begin{cases} k(\mathbf{x},\mathbf{y}) \quad &amp;i = 1\\ k_{i-1}(\mathbf{x},\mathbf{y}) - \frac{k_{i-1}(\mathbf{x},\mathbf{x}_{i-1})k_{i-1}(\mathbf{x}_{i-1},\mathbf{y})}{k_{i-1}(\mathbf{x}_{i-1},\mathbf{x}_{i-1})}\quad &amp;i \geq 2. \end{cases}\] <h2 id="visual-representations">Visual Representations</h2> <h3 id="the-greedy-approach">The Greedy Approach</h3> <p>As the initial residual is just the matrix itself $(\mathbf{K}^{(0)})$ then its diagonal is filled with ones. As such the greedy method will simply pick the first pivot depending on the ordering of the data. This is one of the weaknesses of the greedy method, namely that is susceptible to bad ordering of the data. In fact <d-cite key="chen2024a"></d-cite> gives an example full of outliers. On this example the greedy approach completely fails and gets stuck picking the pivots equal to the ordering of the data. Note that had the ordering of the data in <d-cite key="chen2024a"></d-cite> been random, then implicitly the greedy method would not have failed.</p> <p>In the previously data introduces is completely ordered w.r.t. $x$, and the greedy method therefore picks the first pivot as the point with smallest value. The next iteration again start by looking at the diagonal, but this time of the residual $\mathbf{K}^{(1)}$. Given that the first pivot was chosen as smallest value, it is no surprise that the worst approximation happens furthest away. As such the next pivot point will be at the largest value. The two iterations highlight the weakness of the greedy method: In most cases it ends up picking pivots on the borders of the dataset.</p> <p>The series of the first 4 iterations can be seen below.</p> <h4 id="1st-iteration">1st iteration</h4> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank0_diag-480.webp 480w,/assets/img/continuous/rank0_diag-800.webp 800w,/assets/img/continuous/rank0_diag-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/continuous/rank0_diag.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank1_approx-480.webp 480w,/assets/img/continuous/rank1_approx-800.webp 800w,/assets/img/continuous/rank1_approx-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/continuous/rank1_approx.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h4 id="2nd-iteration">2nd iteration</h4> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank1_diag-480.webp 480w,/assets/img/continuous/rank1_diag-800.webp 800w,/assets/img/continuous/rank1_diag-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/continuous/rank1_diag.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank2_approx-480.webp 480w,/assets/img/continuous/rank2_approx-800.webp 800w,/assets/img/continuous/rank2_approx-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/continuous/rank2_approx.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h4 id="3rd-iteration">3rd iteration</h4> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank2_diag-480.webp 480w,/assets/img/continuous/rank2_diag-800.webp 800w,/assets/img/continuous/rank2_diag-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/continuous/rank2_diag.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank3_approx-480.webp 480w,/assets/img/continuous/rank3_approx-800.webp 800w,/assets/img/continuous/rank3_approx-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/continuous/rank3_approx.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h4 id="4th-iteration">4th iteration</h4> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank3_diag-480.webp 480w,/assets/img/continuous/rank3_diag-800.webp 800w,/assets/img/continuous/rank3_diag-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/continuous/rank3_diag.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank4_approx-480.webp 480w,/assets/img/continuous/rank4_approx-800.webp 800w,/assets/img/continuous/rank4_approx-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/continuous/rank4_approx.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="the-randomized-approach">The Randomized Approach</h3> <p>We now repeat the first 4 steps, but using randomized pivoting approach. A noticeable difference between the greedy and the randomized approach is that the randomized approach prioritizes (with some probability off course) to pick points close to where the data actually is. This is a win, as a point “inside” of the dataset will reduce the overall diagonal of the residual in the places of where data is actually located. In addition, the approach is also completely robust against the ordering of the data, as a randomization of the data is built directly into the approach.</p> <h4 id="1st-iteration-1">1st iteration</h4> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank0_diag_random-480.webp 480w,/assets/img/continuous/rank0_diag_random-800.webp 800w,/assets/img/continuous/rank0_diag_random-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/continuous/rank0_diag_random.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank1_approx_random-480.webp 480w,/assets/img/continuous/rank1_approx_random-800.webp 800w,/assets/img/continuous/rank1_approx_random-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/continuous/rank1_approx_random.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h4 id="2nd-iteration-1">2nd iteration</h4> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank1_diag_random-480.webp 480w,/assets/img/continuous/rank1_diag_random-800.webp 800w,/assets/img/continuous/rank1_diag_random-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/continuous/rank1_diag_random.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank2_approx_random-480.webp 480w,/assets/img/continuous/rank2_approx_random-800.webp 800w,/assets/img/continuous/rank2_approx_random-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/continuous/rank2_approx_random.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h4 id="3rd-iteration-1">3rd iteration</h4> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank2_diag_random-480.webp 480w,/assets/img/continuous/rank2_diag_random-800.webp 800w,/assets/img/continuous/rank2_diag_random-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/continuous/rank2_diag_random.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank3_approx_random-480.webp 480w,/assets/img/continuous/rank3_approx_random-800.webp 800w,/assets/img/continuous/rank3_approx_random-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/continuous/rank3_approx_random.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h4 id="4th-iteration-1">4th iteration</h4> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank3_diag_random-480.webp 480w,/assets/img/continuous/rank3_diag_random-800.webp 800w,/assets/img/continuous/rank3_diag_random-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/continuous/rank3_diag_random.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank4_approx_random-480.webp 480w,/assets/img/continuous/rank4_approx_random-800.webp 800w,/assets/img/continuous/rank4_approx_random-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/continuous/rank4_approx_random.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="comparing-diagonals">Comparing diagonals</h3> <p>To highlight the differences between the two approaches we plot the diagonals together. The overall decay of the diagonal is similar for each method. However, the residual within the dataset is clearly smaller for the randomized approach as compared to the greedy approach. A downside of the randomized approach is that the approximation is not as good “outside” of the data. Said differently, the randomized approach generalizes worse than the greedy approach, by relying on pivots inside, rather than the border, of the dataset.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank1_diags-480.webp 480w,/assets/img/continuous/rank1_diags-800.webp 800w,/assets/img/continuous/rank1_diags-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/continuous/rank1_diags.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank2_diags-480.webp 480w,/assets/img/continuous/rank2_diags-800.webp 800w,/assets/img/continuous/rank2_diags-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/continuous/rank2_diags.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank3_diags-480.webp 480w,/assets/img/continuous/rank3_diags-800.webp 800w,/assets/img/continuous/rank3_diags-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/continuous/rank3_diags.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank4_diags-480.webp 480w,/assets/img/continuous/rank4_diags-800.webp 800w,/assets/img/continuous/rank4_diags-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/continuous/rank4_diags.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="final-remarks">Final remarks</h2> <p>A thing to note about the above is that while the approach only uses pivot points that are in dataset, this not a requirement. That is, a better pivot point could be found outside the dataset. This would be similar to the inducing point approach in sparse Gaussian processes <d-cite key="inducingpoints"></d-cite>, where the inducing points do not have to be part of the dataset.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-05-25-continuous-matrix-factorization.bib"></d-bibliography> <d-article> <br> <br> <div id="giscus_thread"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'mipals/mipals.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Mikkel Paltorp. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?5a16f3fff810d102bb6b8d68a4e2e358"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>