<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://mipals.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://mipals.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-30T17:56:53+00:00</updated><id>https://mipals.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">The Adjoint Method</title><link href="https://mipals.github.io/blog/2025/adjoint-method/" rel="alternate" type="text/html" title="The Adjoint Method"/><published>2025-08-01T12:06:00+00:00</published><updated>2025-08-01T12:06:00+00:00</updated><id>https://mipals.github.io/blog/2025/adjoint-method</id><content type="html" xml:base="https://mipals.github.io/blog/2025/adjoint-method/"><![CDATA[ <p>In many engineering applications we are interested in solving equality constrained optimization problems where the optimization variable ($\theta$) implicitly defines another variable ($u$) that then appears in the objective function<d-footnote>A famous example of this is [topology optimization](https://en.wikipedia.org/wiki/Topology_optimization).</d-footnote>. Written out this means solving optimization problems of the following form</p> \[\begin{equation} \begin{aligned} \min_{\theta \in \mathbb{R}^{n_\theta}} \quad &amp; L(u,\theta), \quad\ \quad\ \left(\text{some objective function}\right)\\ \text{subject to }\quad &amp; f(u,\theta) = 0, \quad \left(\text{some equality constraint}\right) \\ &amp; u \in \mathbb{R}^{n_u}. \end{aligned} \end{equation}\] <p>Most methods for solving equality constrained optimization problems requires the computation of gradient of the objective function with respect to the optimization variable $\theta$. This is where the problem comes in: Naively computing the gradient requires the computation of the sensitivities of the implicitly defined variable $u$ with respect to the optimization variable $\theta$ $\left(\text{i.e.}\ \frac{\mathrm{d}u}{\mathrm{d}\theta} \in \mathbb{R}^{n_u \times n_\theta}\right)$. This result in a computational bottleneck as forming the sensitivities scales as $\mathcal{O}(n_un_\theta)$, meaning that adding a new parameter adds $n_u$ additional sensitivities (and one adding one more variable would add $n_\theta$ additional sensitivities).</p> <p>To resolve this computational bottleneck we can make use the adjoint method. The first step in the derivation of the adjoint method is to introduce the Lagrangian of the objective function, i.e.</p> \[\begin{equation} \mathcal{L}(u,\theta,\lambda) = L(u,\theta) + \lambda^\top f(u,\theta). \end{equation}\] <p>It is easy to see that whenever $u$ satisfy the equality constraint $f(u,\theta)=0$, then the Lagrangian is equal to the original objective function. This in turn mean that the two gradients $\nabla_\theta L(u,\theta,\lambda)$ and $\nabla_\theta \mathcal{L}(u,\theta,\lambda)$ are equal whenever $u$ satisfy the equality constraint $f(u,\theta) = 0$. The reason why the introduction of the additional term in the Lagrangian is useful is that $\lambda$ can be set to be anything. In particular, we aim to set in a way so that we can avoid computing the otherwise computational expensive sensitivities $\frac{\mathrm{d}u}{\mathrm{d}\theta}$. We start by computing the total derivative of the Lagrangian with respect to $\theta$<d-footnote>The total derivative is the transpose of the gradient i.e. $\left(\frac{\mathrm{d}L}{\mathrm{d}\theta}\right)^\top = \nabla_\theta L$. That is multiplying the change in $\theta$ with the total derivative (rather than the transpose of the gradient) gives the change in the function $L$.</d-footnote></p> \[\begin{equation} \frac{\mathrm{d}\mathcal{L}}{\mathrm{d}\theta} = \frac{\partial L}{\partial\theta} + \frac{\partial L}{\partial u}\frac{\mathrm{d}u}{\mathrm{d}\theta} + \lambda^\top\left(\frac{\partial f}{\partial\theta} + \frac{\partial f}{\partial u}\frac{\mathrm{d}u}{\mathrm{d}\theta}\right). \end{equation}\] <p>Now we collect the terms that depend on the undesired $\frac{\mathrm{d}u}{\mathrm{d}\theta}$ result in</p> \[\begin{equation} \frac{\mathrm{d}\mathcal{L}}{\mathrm{d}\theta} = \frac{\partial L}{\partial\theta} + \lambda^\top\frac{\partial f}{\partial\theta} + \left(\frac{\partial L}{\partial u} + \lambda^\top\frac{\partial f}{\partial u}\right)\frac{\mathrm{d}u}{\mathrm{d}\theta} . \end{equation}\] <p>As we can choose $\lambda$ freely a natural idea is to set it in a way such that the term in front of the undesired term vanishes. This means that we can choose $\lambda$ as the solution to the equation</p> \[\begin{equation} \frac{\partial L}{\partial u} + \lambda^\top\frac{\partial f}{\partial u} = 0 \quad \Rightarrow \quad \lambda^\top = -\frac{\partial L}{\partial u}\left(\frac{\partial f}{\partial u}\right)^{-1}. \end{equation}\] <p>Inserting $\lambda^\top$ back into the equation we find that the gradient of the Lagrangian with respect to $\theta$ is given by</p> \[\begin{equation} \frac{\mathrm{d}\mathcal{L}}{\mathrm{d}\theta} = \frac{\partial L}{\partial\theta} \underbrace{- \frac{\partial L}{\partial u}\left(\frac{\partial f}{\partial u}\right)^{-1}}_{\lambda^\top}\frac{\partial f}{\partial\theta} \left(= \frac{\mathrm{d}L}{\mathrm{d}\theta}\right). \end{equation}\] <p>To conclude: The adjoint method is a simple way to avoid the computational bottleneck of computing the sensitivities $\frac{\mathrm{d}u}{\mathrm{d}\theta}$ by cleverly computing the so-called adjoint variable $\lambda$ in a way that eliminates the need to compute the problematic sensitivities.</p> <h3 id="example">Example</h3> <p>In order to illustrate the adjoint method we consider a simple linearly constrained problem of the form (inspiration from problem 38 in <d-cite key="bright2025matrixcalculusformachine"></d-cite>)</p> \[\begin{aligned} L(u,\theta) &amp;= \left(c^\top u(\theta)\right)^2, \\ f(u,\theta) &amp;= A(\theta)u - b = 0, \end{aligned}\] <p>where $A(\theta) \in \mathbb{R}^{n\times n}$ is a symmetric tridiagonal matrix that depends on the parameters $\theta \in \mathbb{R}^{2n-1}$ as</p> \[A = \begin{bmatrix} \theta_1 &amp; \theta_{n+1} &amp; &amp; &amp; 0 \\ \theta_{n+1} &amp; \theta_2 &amp; \ddots &amp; &amp; \\ &amp; \ddots &amp; \ddots &amp; \ddots &amp; \\ &amp; &amp; \ddots &amp; \theta_{n-1} &amp; \theta_{2n-1} \\ 0 &amp; &amp; &amp; \theta_{2n-1} &amp; \theta_n \end{bmatrix}.\] <p>Now in order to compute the gradient of interested we start by computing the adjoint variable $\lambda$ as</p> \[\lambda^\top = -\frac{\partial L}{\partial u}\left(\frac{\partial f}{\partial u}\right)^{-1} = -2\left(c^\top u\right)c^\top A(\theta)^{-1}.\] <p>Note that in practice we do not form $A(\theta)^{-1}$ explicitly but rather compute $\lambda$ by solving the linear system $ A(\theta)^\top\lambda = -2c\left(c^\top u\right)$. Using the adjoint variable we can compute the gradient of $L$ with respect to $\theta$ as</p> \[\frac{\mathrm{d}L}{\mathrm{d}\theta} = \underbrace{\frac{\partial L}{\partial\theta}}_{=0} + \lambda^\top\frac{\partial f}{\partial\theta} = \lambda^\top\frac{\partial A}{\partial\theta}u.\] <p>In the concrete case of the derivatives of $A$ with respect to $\theta_i$ we have that</p> \[\frac{\partial A}{\partial \theta_i} = \begin{bmatrix} \delta_{i,1} &amp; \delta_{i,n+1} &amp; &amp; &amp; 0 \\ \delta_{i,n+1} &amp; \delta_{i,2} &amp; \ddots &amp; &amp; \\ &amp; \ddots &amp; \ddots &amp; \ddots &amp; \\ &amp; &amp; \ddots &amp; \delta_{i,n-1} &amp; \delta_{i,2n-1} \\ 0 &amp; &amp; &amp; \delta_{i,2n-1} &amp; \delta_{i,n} \end{bmatrix}.\] <p>Using this it follows that the $i$th component of the gradient can be computed as</p> \[\lambda^\top\frac{\partial A}{\partial \theta_i}u = \begin{cases} \lambda_i u_i, \quad &amp;i \leq n, \\ \lambda_{i+1-n}u_{i-n} + \lambda_{i-n} u_{i+1-n}, \quad &amp;i &gt; n. \end{cases}\] <details><summary>Code</summary> <div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">LinearAlgebra</span> 
<span class="n">n</span> <span class="o">=</span> <span class="mi">1000</span>        <span class="c"># Number of linear constraints</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">rand</span><span class="x">(</span><span class="n">n</span><span class="x">)</span>     <span class="c"># Objective vector</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">rand</span><span class="x">(</span><span class="n">n</span><span class="x">)</span>     <span class="c"># Linear equality vector</span>
<span class="n">θ</span> <span class="o">=</span> <span class="n">rand</span><span class="x">(</span><span class="mi">2</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="x">)</span>  <span class="c"># Parameters of the equality constraint</span>

<span class="c"># Parametrizing linear equality constraint matrix</span>
<span class="n">A</span><span class="x">(</span><span class="n">θ</span><span class="x">)</span> <span class="o">=</span> <span class="kt">SymTridiagonal</span><span class="x">(</span><span class="n">θ</span><span class="x">[</span><span class="mi">1</span><span class="o">:</span><span class="n">n</span><span class="x">],</span> <span class="n">θ</span><span class="x">[</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="o">:</span><span class="k">end</span><span class="x">])</span> 

<span class="c"># Objective function</span>
<span class="n">f</span><span class="x">(</span><span class="n">θ</span><span class="x">)</span> <span class="o">=</span> <span class="x">(</span><span class="n">c</span><span class="err">'</span> <span class="o">*</span> <span class="x">(</span><span class="n">A</span><span class="x">(</span><span class="n">θ</span><span class="x">)</span> <span class="o">\</span> <span class="n">b</span><span class="x">))</span><span class="o">^</span><span class="mi">2</span> 

<span class="c"># Partial derivatives</span>
<span class="n">partial</span><span class="x">(</span><span class="n">i</span><span class="x">,</span><span class="n">n</span><span class="x">,</span><span class="n">λ</span><span class="x">,</span><span class="n">u</span><span class="x">)</span> <span class="o">=</span> <span class="n">i</span> <span class="o">&lt;=</span> <span class="n">n</span> <span class="o">?</span>  <span class="n">λ</span><span class="x">[</span><span class="n">i</span><span class="x">]</span><span class="o">*</span><span class="n">u</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">:</span> <span class="n">λ</span><span class="x">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="o">-</span><span class="n">n</span><span class="x">]</span><span class="o">*</span><span class="n">u</span><span class="x">[</span><span class="n">i</span><span class="o">-</span><span class="n">n</span><span class="x">]</span> <span class="o">+</span> <span class="n">λ</span><span class="x">[</span><span class="n">i</span><span class="o">-</span><span class="n">n</span><span class="x">]</span><span class="o">*</span><span class="n">u</span><span class="x">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="o">-</span><span class="n">n</span><span class="x">]</span>

<span class="c"># Gradient computation</span>
<span class="k">function</span><span class="nf"> ∇f</span><span class="x">(</span><span class="n">θ</span><span class="x">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">length</span><span class="x">(</span><span class="n">b</span><span class="x">)</span>
    <span class="n">M</span> <span class="o">=</span> <span class="n">A</span><span class="x">(</span><span class="n">θ</span><span class="x">)</span>                <span class="c"># Defining constraint matrix</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">M</span> <span class="o">\</span> <span class="n">b</span>               <span class="c"># Computing solution</span>
    <span class="n">λ</span> <span class="o">=</span> <span class="n">M</span> <span class="o">\</span> <span class="x">(</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">c</span><span class="o">*</span><span class="x">(</span><span class="n">c</span><span class="err">'</span><span class="o">*</span><span class="n">u</span><span class="x">))</span>   <span class="c"># Adjoint variables</span>
    <span class="k">return</span> <span class="x">[</span><span class="n">partial</span><span class="x">(</span><span class="n">i</span><span class="x">,</span><span class="n">n</span><span class="x">,</span><span class="n">λ</span><span class="x">,</span><span class="n">u</span><span class="x">)</span> <span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="o">:</span><span class="mi">2</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="x">]</span>
<span class="k">end</span>

<span class="c"># Testing against ForwardDiff</span>
<span class="k">using</span> <span class="n">Test</span><span class="x">,</span> <span class="n">ForwardDiff</span>
<span class="nd">@test</span> <span class="n">ForwardDiff</span><span class="o">.</span><span class="n">gradient</span><span class="x">(</span><span class="n">f</span><span class="x">,</span><span class="n">θ</span><span class="x">)</span> <span class="n">≈</span> <span class="n">∇f</span><span class="x">(</span><span class="n">θ</span><span class="x">)</span>
</code></pre></div></div> </details> ]]></content><author><name>Mikkel Paltorp</name></author><category term="optimization"/><category term="constrained-optimization"/><category term="adjoint-method"/><summary type="html"><![CDATA[and why its easier than people make it out to be]]></summary></entry><entry><title type="html">Associative Scan</title><link href="https://mipals.github.io/blog/2025/associative-scan/" rel="alternate" type="text/html" title="Associative Scan"/><published>2025-07-30T12:06:00+00:00</published><updated>2025-07-30T12:06:00+00:00</updated><id>https://mipals.github.io/blog/2025/associative-scan</id><content type="html" xml:base="https://mipals.github.io/blog/2025/associative-scan/"><![CDATA[<p>Recently a plethora of methods within machine learning have made use of state-space models (SSMs) to model sequences<d-cite key="gu2022efficientlymodelinglongsequences,gu2024mambalineartimesequencemodeling,dao2024a"></d-cite>. A big reason for their success is their usage of the associative scan operation to parallelize the state computation on the GPU. In my opinion this is a neat trick that was poorly explained in the original papers (although Appendix B in <d-cite key="dao2024a"></d-cite> does explain it for the scalar case), and I will therefore in this blog post explain it in more detail. The aim is to show that the transition of a (linear) state-space model</p> \[\begin{equation} \begin{aligned} \bh_i &amp;= \bA_i \bh_{i-1} + \bB_i \bx_i, \\ \by_i &amp;= \bC_i \bh_i \end{aligned}, \end{equation}\] <p>is associative<d-footnote>Associativity simply mean that we can apply the operator (denoted by $\bullet$) in arbitrary order, i.e. that $a \bullet (b \bullet c) = (a \bullet b) \bullet c$. Examples of associative operators are addition and multiplication.</d-footnote>. In short this means that the states of a (linear) state-space model up until time step $T$ can be computed in parallel using an associative scan operation. For details on the associative operation open the example box below.</p> <details><summary>Example: Cumulative sum using associative scan</summary> <p>The (parallel) associative scan, as the name suggest, is a way to apply an associative operator in parallel. The simplest of such operator is addition, which is associative because we can switch around the order of computation (i.e $a + (b + c) = (a + b) + c$). In Jax the associative scan is implemented in the accurately named <em>lax.associative_scan</em> function while in Julia it is denoted by the <em>accumulate</em> function. If we supply the add operator the resulting computation will be equivalent with the cumulative sum, e.g. in Python</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">lax</span><span class="p">.</span><span class="nf">associative_scan</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="n">add</span><span class="p">,</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="nc">Array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">int32</span><span class="p">)</span>
</code></pre></div></div> <p>or in Julia</p> <div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">julia</span><span class="o">&gt;</span> <span class="n">accumulate</span><span class="x">(</span><span class="o">+</span><span class="x">,</span> <span class="mi">0</span><span class="o">:</span><span class="mi">3</span><span class="x">)</span>
<span class="mi">5</span><span class="o">-</span><span class="n">element</span> <span class="kt">Vector</span><span class="x">{</span><span class="kt">Int64</span><span class="x">}</span><span class="o">:</span>
  <span class="mi">0</span>
  <span class="mi">1</span>
  <span class="mi">3</span>
  <span class="mi">6</span>
</code></pre></div></div> <p>While the above computation is trivial and easily computed by looping from the first to last element in the vector, the main idea of the associative scan is that the operator can be applied pairwise in parallel. That means if enough processors is available the computation can be done in $O(\log n)$ time rather than the trivial implementation of $O(n)$.</p> </details> <p>The first step in showing the associativity of the state-space model is to define the transition of the state-space models using matrix multiplication (which is associative) by embedding the transition into a larger matrix $\bs_k$ as follows</p> \[\bs_k = \begin{bmatrix} \bA_k &amp; \bB_k \bx_k \\ \bzero &amp; 1 \end{bmatrix}, \quad \bs_0 = \begin{bmatrix} \bzero &amp; \bh_0 \\ \bzero &amp; 1 \end{bmatrix}\] <p>Using the definition of $\bs_k$ the state transition from state $k-1$ to state $k$ can be computed using matrix multiplication as</p> \[\bs_k\bs_{k-1} = \begin{bmatrix} \bA_k\bA_{k-1} &amp; \bA_k(\bB_{k-1}\bx_{k-1}) + \bB_k\bx_k \\ \bzero &amp; 1 \end{bmatrix}.\] <p>Using this we can compute the $i$th state of the state-space model as </p> \[\begin{equation} \bx_i = \begin{bmatrix}\bI &amp; \bzero \end{bmatrix} \left(\prod_{k=i}^0 \bs_k\right) \begin{bmatrix}\bzero \\ 1\end{bmatrix}. \end{equation}\] <p>Given that the cumulative product can be computed using the associative scan operator the full dynamics can be computed as</p> \[\begin{equation} \begin{aligned} \bp_i &amp;= \text{associative_scan}(\bs_i, \text{init} = \bs_0)\\ \by_i &amp;= \bC \bh_i = \begin{bmatrix}\bC &amp; \bzero \end{bmatrix} \bp_i \begin{bmatrix}\bzero \\ 1\end{bmatrix}. \end{aligned} \end{equation}\] <p>While the above works, it can be simplified slightly. As we are really only interested in what happens in top block (as the top right block contain $\bx_i$) we can instead define elements by just the top row, i.e. instead define the states as $\bs_k = \begin{bmatrix}\bA_k &amp; \bB_k \bx_k \end{bmatrix}$ and then define the associative operator (denoted by $\bullet$) by how the top row propagates, i.e.</p> \[\begin{equation} \bs_k \bullet \bs_{k-1} = \begin{bmatrix} \bA_k \bA_{k-1} &amp; \bA_k (\bB_{k-1} \bx_{k-1}) + \bB_k \bx_k\end{bmatrix}. \end{equation}\] <p>The SSM stages can then be computed as $\bp_i = \bs_i \bullet \bp_{i-1}$ with $\bs_0 = \begin{bmatrix} \bzero &amp; \bh_0 \end{bmatrix}$.</p> <p>As a final remark note that while the associative scan is parallelizable it performs matrix-matrix products of the form $\bA_k \bA_{k-1}$ which will be computational prohibitive unless $\bA_k$ has structure (e.g. diagonal or low-rank). This is one of the reasons why e.g. Mamba-2 utilizes a scaled identity as its $\bA_k$ <d-cite key="dao2024a"></d-cite>.</p>]]></content><author><name>Mikkel Paltorp</name></author><category term="linear-algebra"/><category term="state-space-models"/><category term="semiseparable-matrices"/><summary type="html"><![CDATA[and its state-space models]]></summary></entry><entry><title type="html">Block Tridiagonal Matrices</title><link href="https://mipals.github.io/blog/2025/block-tridiagonal/" rel="alternate" type="text/html" title="Block Tridiagonal Matrices"/><published>2025-07-28T15:06:00+00:00</published><updated>2025-07-28T15:06:00+00:00</updated><id>https://mipals.github.io/blog/2025/block-tridiagonal</id><content type="html" xml:base="https://mipals.github.io/blog/2025/block-tridiagonal/"><![CDATA[<p>We will in this post restrict ourselves to the case of symmetric block tridiagonal matrices. These matrices occur in the context of state space models, since the state transition matrix is lower block bidiagonal resulting in the covariance matrix being symmetric block tridiagonal. It turns out that the inverse of a symmetric block tridiagonal matrix is semiseparable with separability rank equal to the size of the blocks <d-cite key="meurant1992a"></d-cite>. We introduce the following notation for a symmetric block tridiagonal matrix</p> \[\bK = \begin{bmatrix} \bD_1 &amp; -\bA_2^\top &amp; &amp; &amp; \\ -\bA_2 &amp; \bD_2 &amp; -\bA_3^\top &amp; &amp; \\ &amp; \ddots &amp; \ddots &amp; \ddots &amp; \\ &amp; &amp; -\bA_{N-1} &amp; \bD_{N-1} &amp; -\bA_{N}^\top\\ &amp; &amp; &amp; -\bA_T &amp; \bD_T \end{bmatrix}.\] <p>It turns out that the matrix can be factorized in two ways as follows</p> \[\bK = \left(\Delta + \bL\right)\Delta^{-1}\left(\Delta + \bL^\top\right) = \left(\Sigma + \bL^\top\right)\Sigma^{-1}\left(\Sigma + \bL\right) ,\] <p>where $\bL$ is the matrix of lower triangular blocks and $\Delta$ and $\Sigma$ are block diagonal matrices with blocks computed as follows</p> \[\begin{cases} \Delta_1 = \bD_1, \\ \Delta_i = \bD_i - \bA_i\Delta_{i-1}^{-1}\bA_i^\top \end{cases}, \quad \begin{cases} \Sigma_T = \bD_T, \\ \Sigma_i = \bD_i - \bA_{i+1}^\top\Sigma_{i+1}^{-1}\bA_{i+1} \end{cases}.\] <p>In the case of the of diagonal blocks ($\bA_i$) being invertible, and using $\Delta$ and $\Sigma$, then $\bK^{-1}$ is semiseparable generator representable. In short this means that the inverse of $\bK$ can be written as</p> \[\bK^{-1} = \text{tril}\left(\bU\bV^\top\right) + \text{triu}\left(\bV\bU^\top,1\right).\] <p>where the so-called generators $\bU$ and $\bV$ can be computed as</p> \[\label{eq:generators} \begin{cases} \bU_1 = \Sigma_1^{-1}\\ \bU_i = \Sigma_{i}^{-1}\bA_i\bU_{i-1} \end{cases}, \quad \begin{cases} \bV_1 = \bI\\ \bV_i = \bA_i^{-\top}\Delta_{i-1}\bV_{i-1} \end{cases},\] <p>The semiseparable representation extremely important computationally. We have that $\bU,\bV\in\mathbb{R}^{nT\times n}$ which mean that it is a data-sparse representation of $\bK^{-1}\in\mathbb{R}^{nT\times nT}$ (we have $D_i \in \mathbb{R}^{n\times n}$ and $N$ being the number of states. In most cases $n \ll N$ meaning that the representation is extremely efficient). At the same time the structure make is possible to compute various important linear algebraic operation in $O(n^2T)$ that in the dense case would have scaled as $O((nT)^3)$ <d-cite key="andersen2020a"></d-cite>.</p> <p>Some Julia code implementing the above is given below.</p> <details><summary>Code</summary> <div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">LinearAlgebra</span><span class="x">,</span> <span class="n">FillArrays</span><span class="x">,</span> <span class="n">LinearAlgebra</span><span class="x">,</span> <span class="n">BlockBandedMatrices</span><span class="x">,</span> <span class="n">Test</span>

<span class="n">T</span> <span class="o">=</span> <span class="mi">5</span> <span class="c"># Sequence length</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">3</span> <span class="c"># blk sizes</span>

<span class="n">A_blks</span> <span class="o">=</span> <span class="x">[</span><span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="o">?</span> <span class="n">zeros</span><span class="x">(</span><span class="n">n</span><span class="x">,</span><span class="n">n</span><span class="x">)</span> <span class="o">:</span> <span class="n">exp</span><span class="x">(</span><span class="n">rand</span><span class="x">(</span><span class="n">n</span><span class="x">,</span><span class="n">n</span><span class="x">))</span> <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">T</span><span class="x">]</span> 
<span class="n">D_blks</span> <span class="o">=</span> <span class="x">[</span><span class="kt">Matrix</span><span class="x">(</span><span class="mf">1.0</span><span class="o">*</span><span class="n">I</span><span class="x">(</span><span class="n">n</span><span class="x">))</span> <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">T</span><span class="x">]</span>
<span class="n">A_blks_transposed</span> <span class="o">=</span> <span class="x">[</span><span class="n">A_blk</span><span class="err">'</span> <span class="k">for</span> <span class="n">A_blk</span> <span class="k">in</span> <span class="n">A_blks</span><span class="x">]</span>
<span class="n">zero_blks</span> <span class="o">=</span> <span class="x">[</span><span class="n">zeros</span><span class="x">(</span><span class="n">n</span><span class="x">,</span><span class="n">n</span><span class="x">)</span> <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">T</span><span class="x">]</span>

<span class="n">K</span> <span class="o">=</span> <span class="n">BlockTridiagonal</span><span class="x">(</span><span class="o">-</span><span class="n">A_blks</span><span class="x">[</span><span class="mi">2</span><span class="o">:</span><span class="k">end</span><span class="x">],</span> <span class="n">D_blks</span><span class="x">,</span> <span class="o">-</span><span class="n">A_blks_transposed</span><span class="x">[</span><span class="mi">2</span><span class="o">:</span><span class="k">end</span><span class="x">])</span>
<span class="n">Δ_blks</span> <span class="o">=</span> <span class="n">copy</span><span class="x">(</span><span class="n">D_blks</span><span class="x">)</span>
<span class="n">Σ_blks</span> <span class="o">=</span> <span class="n">copy</span><span class="x">(</span><span class="n">D_blks</span><span class="x">)</span>
<span class="k">for</span> <span class="x">(</span><span class="n">i</span><span class="x">,</span><span class="n">j</span><span class="x">)</span> <span class="k">in</span> <span class="n">zip</span><span class="x">(</span><span class="mi">2</span><span class="o">:</span><span class="n">length</span><span class="x">(</span><span class="n">D_blks</span><span class="x">),</span><span class="n">length</span><span class="x">(</span><span class="n">D_blks</span><span class="x">)</span><span class="o">:-</span><span class="mi">1</span><span class="o">:</span><span class="mi">2</span><span class="x">)</span>
    <span class="n">Δ_blks</span><span class="x">[</span><span class="n">i</span><span class="x">]</span>   <span class="o">=</span> <span class="n">D_blks</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">-</span> <span class="n">A_blks</span><span class="x">[</span><span class="n">i</span><span class="x">]</span><span class="o">*</span><span class="x">(</span><span class="n">Δ_blks</span><span class="x">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="x">]</span><span class="o">\</span><span class="n">A_blks</span><span class="x">[</span><span class="n">i</span><span class="x">]</span><span class="err">'</span><span class="x">)</span>
    <span class="n">Σ_blks</span><span class="x">[</span><span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="x">]</span> <span class="o">=</span> <span class="n">D_blks</span><span class="x">[</span><span class="n">j</span><span class="x">]</span> <span class="o">-</span> <span class="n">A_blks</span><span class="x">[</span><span class="n">j</span><span class="x">]</span><span class="err">'</span><span class="o">*</span><span class="x">(</span><span class="n">Σ_blks</span><span class="x">[</span><span class="n">j</span><span class="x">]</span><span class="o">\</span><span class="n">A_blks</span><span class="x">[</span><span class="n">j</span><span class="x">])</span>
<span class="k">end</span>

<span class="n">L</span> <span class="o">=</span> <span class="n">BlockTridiagonal</span><span class="x">(</span><span class="o">-</span><span class="n">A_blks</span><span class="x">[</span><span class="mi">2</span><span class="o">:</span><span class="k">end</span><span class="x">],</span> <span class="n">zero_blks</span><span class="x">,</span> <span class="n">zero_blks</span><span class="x">[</span><span class="mi">1</span><span class="o">:</span><span class="k">end</span><span class="o">-</span><span class="mi">1</span><span class="x">])</span>
<span class="n">Δ</span> <span class="o">=</span> <span class="n">BlockTridiagonal</span><span class="x">(</span><span class="n">zero_blks</span><span class="x">[</span><span class="mi">1</span><span class="o">:</span><span class="k">end</span><span class="o">-</span><span class="mi">1</span><span class="x">],</span> <span class="n">Δ_blks</span><span class="x">,</span> <span class="n">zero_blks</span><span class="x">[</span><span class="mi">1</span><span class="o">:</span><span class="k">end</span><span class="o">-</span><span class="mi">1</span><span class="x">])</span>
<span class="n">Σ</span> <span class="o">=</span> <span class="n">BlockTridiagonal</span><span class="x">(</span><span class="n">zero_blks</span><span class="x">[</span><span class="mi">1</span><span class="o">:</span><span class="k">end</span><span class="o">-</span><span class="mi">1</span><span class="x">],</span> <span class="n">Σ_blks</span><span class="x">,</span> <span class="n">zero_blks</span><span class="x">[</span><span class="mi">1</span><span class="o">:</span><span class="k">end</span><span class="o">-</span><span class="mi">1</span><span class="x">])</span>

<span class="nd">@test</span> <span class="x">(</span><span class="n">Δ</span> <span class="o">+</span> <span class="n">L</span><span class="x">)</span><span class="o">*</span><span class="x">(</span><span class="n">Δ</span><span class="o">\</span><span class="x">(</span><span class="n">Δ</span> <span class="o">+</span> <span class="n">L</span><span class="err">'</span><span class="x">))</span> <span class="n">≈</span> <span class="n">K</span>
<span class="nd">@test</span> <span class="x">(</span><span class="n">Σ</span> <span class="o">+</span> <span class="n">L</span><span class="err">'</span><span class="x">)</span><span class="o">*</span><span class="x">(</span><span class="n">Σ</span><span class="o">\</span><span class="x">(</span><span class="n">Σ</span> <span class="o">+</span> <span class="n">L</span><span class="x">))</span> <span class="n">≈</span> <span class="n">K</span>

<span class="c"># Creating the semiseperable form</span>
<span class="n">V_blk</span> <span class="o">=</span> <span class="x">[</span><span class="kt">Matrix</span><span class="x">(</span><span class="mf">1.0</span><span class="o">*</span><span class="n">I</span><span class="x">(</span><span class="n">n</span><span class="x">))</span> <span class="k">for</span> <span class="n">_</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">T</span><span class="x">]</span>
<span class="n">U_blk</span> <span class="o">=</span> <span class="x">[</span><span class="kt">Matrix</span><span class="x">(</span><span class="mf">1.0</span><span class="o">*</span><span class="n">I</span><span class="x">(</span><span class="n">n</span><span class="x">))</span> <span class="k">for</span> <span class="n">_</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">T</span><span class="x">]</span>
<span class="n">U_blk</span><span class="x">[</span><span class="mi">1</span><span class="x">]</span> <span class="o">=</span> <span class="n">inv</span><span class="x">(</span><span class="n">Σ_blks</span><span class="x">[</span><span class="mi">1</span><span class="x">])</span>
<span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">2</span><span class="o">:</span><span class="n">T</span>
    <span class="n">U_blk</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">=</span> <span class="n">Σ_blks</span><span class="x">[</span><span class="n">i</span><span class="x">]</span><span class="err">'</span><span class="o">\</span><span class="n">A_blks</span><span class="x">[</span><span class="n">i</span><span class="x">]</span><span class="o">*</span><span class="n">U_blk</span><span class="x">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="x">]</span>
    <span class="n">V_blk</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">=</span> <span class="n">A_blks</span><span class="x">[</span><span class="n">i</span><span class="x">]</span><span class="err">'</span><span class="o">\</span><span class="x">(</span><span class="n">Δ_blks</span><span class="x">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="x">]</span><span class="o">*</span><span class="n">V_blk</span><span class="x">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="x">])</span>
<span class="k">end</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">vcat</span><span class="x">(</span><span class="n">U_blk</span><span class="o">...</span><span class="x">)</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">vcat</span><span class="x">(</span><span class="n">V_blk</span><span class="o">...</span><span class="x">)</span>
<span class="nd">@test</span> <span class="n">tril</span><span class="x">(</span><span class="n">U</span><span class="o">*</span><span class="n">V</span><span class="err">'</span><span class="x">)</span> <span class="n">≈</span> <span class="n">tril</span><span class="x">(</span><span class="n">inv</span><span class="x">(</span><span class="n">K</span><span class="x">))</span>
<span class="nd">@test</span> <span class="n">triu</span><span class="x">(</span><span class="n">V</span><span class="o">*</span><span class="n">U</span><span class="err">'</span><span class="x">)</span> <span class="n">≈</span> <span class="n">triu</span><span class="x">(</span><span class="n">inv</span><span class="x">(</span><span class="n">K</span><span class="x">))</span>
</code></pre></div></div> </details> ]]></content><author><name>Mikkel Paltorp</name></author><category term="block-tridiagonal"/><category term="linear-algebra"/><category term="state-space-models"/><category term="semiseparable-matrices"/><summary type="html"><![CDATA[and their relation to semiseparable matrices and state-space models]]></summary></entry><entry><title type="html">Structured Masked Attention</title><link href="https://mipals.github.io/blog/2025/structured-masked-attention/" rel="alternate" type="text/html" title="Structured Masked Attention"/><published>2025-07-28T15:06:00+00:00</published><updated>2025-07-28T15:06:00+00:00</updated><id>https://mipals.github.io/blog/2025/structured-masked-attention</id><content type="html" xml:base="https://mipals.github.io/blog/2025/structured-masked-attention/"><![CDATA[<p>Masked Attention (MA) is given by the relation<d-cite key="dao2024a"></d-cite></p> \[\bM = \bB \odot \bL,\] <p>where $\bL$ is a mask applied to the matrix $\bB$ using the elementwise Hadamard product (denoted by $\odot$). When the mask is <em>structured</em> one can most often apply multiplication with $\bM$ efficiently, and we refer to the masked attention as Structured Masked Attention (SMA). In the simplest case of $\bL$ being a lower triangular matrix filled with ones the SMA reduces to</p> \[\bM = \bB \odot \bL = \tril(\bB),\] <p>which can be viewed as a weighted cumulative sum. While the above is a <em>structured</em> computation, it is not efficient unless $\bB$ itself have some structure. An example of when $\bB$ is structured is when it is of low-rank (i.e. $\bB = \bU\bV^\top$). The resulting SMA is a semiseparable matrix for which multiplication can be applied in linear time<d-cite key="andersen2020a"></d-cite>.</p> <p>In general if $\bL$ is a low-rank matrix (of rank $p$) then multiplication with $\bM$ scales as $p$-times the scaling of multiplication with $\bB$ as</p> \[\bM\bx = \left(\bB \odot \bU\bV^\top\right)\bx = \sum_{i=1}^n \diag(\bu_i)\bB \diag(\bv_i)\bx,\] <p>for which we see that we need to perform $p$ multiplications with $\bB$ as well as $2p$ diagonal multiplications.</p> <details><summary>Code</summary> <div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">LinearAlgebra</span><span class="x">,</span> <span class="n">Test</span>
<span class="n">n</span><span class="x">,</span> <span class="n">p</span> <span class="o">=</span> <span class="mi">10</span><span class="x">,</span> <span class="mi">2</span>
<span class="n">U</span><span class="x">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">randn</span><span class="x">(</span><span class="n">n</span><span class="x">,</span><span class="n">p</span><span class="x">),</span> <span class="n">randn</span><span class="x">(</span><span class="n">n</span><span class="x">,</span><span class="n">p</span><span class="x">)</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">rand</span><span class="x">(</span><span class="n">n</span><span class="x">,</span><span class="n">n</span><span class="x">)</span>
<span class="n">M</span> <span class="o">=</span> <span class="n">B</span> <span class="o">.*</span> <span class="x">(</span><span class="n">U</span><span class="o">*</span><span class="n">V</span><span class="err">'</span><span class="x">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">randn</span><span class="x">(</span><span class="n">n</span><span class="x">)</span>
<span class="nd">@test</span> <span class="n">M</span><span class="o">*</span><span class="n">x</span> <span class="n">≈</span> <span class="n">sum</span><span class="x">(</span><span class="n">i</span> <span class="o">-&gt;</span> <span class="kt">Diagonal</span><span class="x">(</span><span class="n">U</span><span class="x">[</span><span class="o">:</span><span class="x">,</span><span class="n">i</span><span class="x">])</span><span class="o">*</span><span class="x">(</span><span class="n">B</span><span class="o">*</span><span class="x">(</span><span class="kt">Diagonal</span><span class="x">(</span><span class="n">V</span><span class="x">[</span><span class="o">:</span><span class="x">,</span><span class="n">i</span><span class="x">]))</span> <span class="o">*</span> <span class="n">x</span><span class="x">),</span><span class="mi">1</span><span class="o">:</span><span class="n">p</span><span class="x">)</span>
</code></pre></div></div> </details> <h2 id="state-space-models-as-structured-matrices">State-space models as structured matrices</h2> <p>First we recap that a state-space model is given by the equations</p> \[\begin{equation} \begin{aligned} \bh_i &amp;= \bA_i \bh_{i-1} + \bB_i \bx_i, \\ \by_i &amp;= \bC_i \bh_i \end{aligned}, \end{equation}\] <p>An alternative way to view the state-space model is through the lens of structured matrices. In particular a state-space model up until time step $T$ can also be written using block matrices as well as block bidiagonal matrices as</p> \[\begin{equation} \underbrace{\begin{bmatrix} \bI &amp; &amp; &amp; \\ -\bA_1 &amp; \bI &amp; &amp; \\ &amp; \ddots &amp; \ddots &amp; \\ &amp; &amp; -\bA_{T-1} &amp; \bI \end{bmatrix}}_{\bA} \begin{bmatrix} \bh_0 \\ \bh_1 \\ \vdots \\ \bh_{T-1} \end{bmatrix} = \begin{bmatrix} \bB_0\bx_0 \\ \bB_1\bx_1 \\ \vdots \\ \bB_{T-1}\bx_{T-1} \end{bmatrix} = \underbrace{\blkdiag\left(\begin{bmatrix} \bB_0 \\ \bB_1 \\ \vdots \\ \bB_{T-1} \end{bmatrix}\right)}_{\bB} \begin{bmatrix} \bx_0 \\ \bx_1 \\ \vdots \\ \bx_{T-1} \end{bmatrix} \end{equation}\] <p>We know that by simply iterating forwards in time we can compute the states in linear time. As such it is not a surprise that the inverse of the bidiagonal matrix $\bA$ (i.e. $\bA^{-1}$) can be computed in linear time as</p> \[\begin{equation} \bA^{-1} = \tril\left(\underbrace{\begin{bmatrix}\bI \\ \bA_1 \\ \bA_2 \bA_1 \\ \vdots \\ \bA_{T-1}\dots \bA_1\end{bmatrix}}_{\bU}\underbrace{\begin{bmatrix}\bI \\ \bA_1^{-1} \\ (\bA_2 \bA_1)^{-1} \\ \vdots \\ (\bA_{T-1}\dots \bA_1)^{-1}\end{bmatrix}^\top}_{\bV^\top}\right), \end{equation}\] <p>This type of matrix structure is called semiseparable <d-cite key="andersen2020a"></d-cite>. Using the explicit inverse of $\bA$ we can compute the hidden states efficiently as</p> \[\begin{equation} \begin{bmatrix} \bh_0 \\ \bh_1 \\ \vdots \\ \bh_{T-1} \end{bmatrix} = \underbrace{\tril\left(\begin{bmatrix}\bI \\ \bA_1 \\ \bA_2 \bA_1 \\ \vdots \\ \bA_{T-1}\dots \bA_1\end{bmatrix}\begin{bmatrix}\bI \\ \bA_1^{-1} \\ (\bA_2 \bA_1)^{-1} \\ \vdots \\ (\bA_{T-1}\dots \bA_1)^{-1}\end{bmatrix}^\top\right)}_{\bA^{-1}} \blkdiag\left(\begin{bmatrix} \bB_0 \\ \bB_1 \\ \vdots \\ \bB_{T-1} \end{bmatrix}\right) \begin{bmatrix} \bx_0 \\ \bx_1 \\ \vdots \\ \bx_{T-1} \end{bmatrix}, \end{equation}\] <p>Similarly, the output $\by$ can be computed by applying the output matrices $\bC_i$ in a blocked fashion to the hidden states, i.e.</p> \[\begin{equation} \begin{bmatrix} \by_1 \\ \by_2 \\ \vdots \\ \by_T \end{bmatrix} = \underbrace{ \overbrace{\blkdiag\left(\begin{bmatrix} \bC_0^\top \\ \bC_1^\top \\ \vdots \\ \bC_{T-1}^\top \end{bmatrix}\right)}^{\bC^\top} \begin{bmatrix} \bI &amp; &amp; &amp; \\ -\bA_1 &amp; \bI &amp; &amp; \\ &amp; \ddots &amp; \ddots &amp; \\ &amp; &amp; -\bA_{T-1} &amp; \bI \end{bmatrix}^{-1} \blkdiag\left(\begin{bmatrix} \bB_0 \\ \bB_1 \\ \vdots \\ \bB_{T-1} \end{bmatrix}\right)}_{\bM} \begin{bmatrix} \bx_1 \\ \bx_2 \\ \vdots \\ \bx_T \end{bmatrix} \end{equation}\] <details><summary>Code</summary> <div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">LinearAlgebra</span><span class="x">,</span> <span class="n">BlockBandedMatrices</span><span class="x">,</span> <span class="n">Test</span><span class="x">,</span> <span class="n">SymSemiseparableMatrices</span>

<span class="n">T</span> <span class="o">=</span> <span class="mi">10</span>          <span class="c"># Sequence length</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">6</span>           <span class="c"># State size</span>
<span class="n">input_dim</span> <span class="o">=</span> <span class="mi">1</span>   <span class="c"># Dimension of forcing term</span>

<span class="n">A_blks</span> <span class="o">=</span> <span class="x">[</span><span class="n">rand</span><span class="x">()</span><span class="o">*</span><span class="n">I</span><span class="x">(</span><span class="n">n</span><span class="x">)</span> <span class="k">for</span> <span class="n">_</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">T</span><span class="o">-</span><span class="mi">1</span><span class="x">]</span>       <span class="c"># Diagonal dynamics in Mamba 2</span>
<span class="n">D_blks</span> <span class="o">=</span> <span class="x">[</span><span class="n">I</span><span class="x">(</span><span class="n">n</span><span class="x">)</span><span class="o">/</span><span class="mf">1.0</span> <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">T</span><span class="x">]</span>            <span class="c"># Diagonal blocks are identity</span>
<span class="n">C_blks</span> <span class="o">=</span> <span class="x">[</span><span class="n">rand</span><span class="x">(</span><span class="n">n</span><span class="x">,</span><span class="mi">1</span><span class="x">)</span> <span class="k">for</span> <span class="n">_</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">T</span><span class="x">]</span>           <span class="c"># Measurements are scalars</span>
<span class="n">B_blks</span> <span class="o">=</span> <span class="x">[</span><span class="n">rand</span><span class="x">(</span><span class="n">n</span><span class="x">,</span><span class="n">input_dim</span><span class="x">)</span> <span class="k">for</span> <span class="n">_</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">T</span><span class="x">]</span>   <span class="c"># Inputs are scalars</span>
<span class="c"># Defining zeros blocks for the matrices</span>
<span class="n">A_zero_blks</span> <span class="o">=</span> <span class="x">[</span><span class="n">zeros</span><span class="x">(</span><span class="n">n</span><span class="x">,</span><span class="n">n</span><span class="x">)</span> <span class="k">for</span> <span class="n">b</span> <span class="k">in</span> <span class="n">A_blks</span><span class="x">]</span>
<span class="n">C_zero_blks</span> <span class="o">=</span> <span class="x">[</span><span class="n">zeros</span><span class="x">(</span><span class="n">n</span><span class="x">,</span><span class="mi">1</span><span class="x">)</span> <span class="k">for</span> <span class="n">_</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">T</span><span class="o">-</span><span class="mi">1</span><span class="x">]</span>
<span class="n">B_zero_blks</span> <span class="o">=</span> <span class="x">[</span><span class="n">zeros</span><span class="x">(</span><span class="n">n</span><span class="x">,</span><span class="n">input_dim</span><span class="x">)</span> <span class="k">for</span> <span class="n">_</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">T</span><span class="o">-</span><span class="mi">1</span><span class="x">]</span>   
<span class="c"># Defining the block matrices</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">BlockTridiagonal</span><span class="x">(</span><span class="o">-</span><span class="n">A_blks</span><span class="x">,</span><span class="n">D_blks</span><span class="x">,</span><span class="n">A_zero_blks</span><span class="x">)</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">BlockTridiagonal</span><span class="x">(</span><span class="n">C_zero_blks</span><span class="x">,</span><span class="n">C_blks</span><span class="x">,</span><span class="n">C_zero_blks</span><span class="x">)</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">BlockTridiagonal</span><span class="x">(</span><span class="n">B_zero_blks</span><span class="x">,</span><span class="n">B_blks</span><span class="x">,</span><span class="n">B_zero_blks</span><span class="x">)</span>

<span class="c"># Computing states by iterating forward</span>
<span class="n">x_blks</span> <span class="o">=</span> <span class="x">[</span><span class="n">randn</span><span class="x">(</span><span class="n">input_dim</span><span class="x">)</span> <span class="k">for</span> <span class="n">_</span> <span class="k">in</span> <span class="mi">0</span><span class="o">:</span><span class="n">T</span><span class="o">-</span><span class="mi">1</span><span class="x">]</span>  <span class="c"># input data</span>
<span class="n">h_blks</span> <span class="o">=</span> <span class="x">[</span><span class="n">B_blks</span><span class="x">[</span><span class="mi">1</span><span class="x">]</span><span class="o">*</span><span class="n">x_blks</span><span class="x">[</span><span class="mi">1</span><span class="x">]]</span>              <span class="c"># initial hidden state</span>
<span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">2</span><span class="o">:</span><span class="n">T</span>
    <span class="n">push!</span><span class="x">(</span><span class="n">h_blks</span><span class="x">,</span> <span class="n">A_blks</span><span class="x">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="x">]</span><span class="o">*</span><span class="n">h_blks</span><span class="x">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="x">]</span> <span class="o">+</span> <span class="n">B_blks</span><span class="x">[</span><span class="n">i</span><span class="x">]</span><span class="o">*</span><span class="n">x_blks</span><span class="x">[</span><span class="n">i</span><span class="x">])</span>
<span class="k">end</span>
<span class="n">y_blks</span> <span class="o">=</span> <span class="x">[</span><span class="n">C</span><span class="err">'</span><span class="o">*</span><span class="n">h</span> <span class="k">for</span> <span class="x">(</span><span class="n">C</span><span class="x">,</span><span class="n">h</span><span class="x">)</span> <span class="k">in</span> <span class="n">zip</span><span class="x">(</span><span class="n">C_blks</span><span class="x">,</span><span class="n">h_blks</span><span class="x">)]</span> 

<span class="c"># Computing states using semiseparable matrices</span>
<span class="n">Ai_blks</span> <span class="o">=</span> <span class="x">[</span><span class="n">prod</span><span class="x">(</span><span class="n">A_blks</span><span class="x">[</span><span class="mi">1</span><span class="o">:</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="x">],</span><span class="n">init</span><span class="o">=</span><span class="mf">1.0</span><span class="o">*</span><span class="n">I</span><span class="x">(</span><span class="n">n</span><span class="x">))</span> <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">T</span><span class="x">]</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">vcat</span><span class="x">(</span><span class="n">Ai_blks</span><span class="o">...</span><span class="x">)</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">vcat</span><span class="x">(</span><span class="n">inv</span><span class="o">.</span><span class="x">(</span><span class="n">Ai_blks</span><span class="x">)</span><span class="o">...</span><span class="x">)</span>
<span class="c"># "SymSemiseparableCholesky" represents the matrix A^{-1} = tril(UV') </span>
<span class="n">Ai</span> <span class="o">=</span> <span class="n">SymSemiseparableCholesky</span><span class="x">(</span><span class="n">U</span><span class="err">'</span><span class="x">,</span><span class="n">V</span><span class="err">'</span><span class="x">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">vcat</span><span class="x">(</span><span class="n">x_blks</span><span class="o">...</span><span class="x">)</span> <span class="c"># Collecting input data</span>

<span class="nd">@test</span> <span class="n">Ai</span><span class="o">*</span><span class="x">(</span><span class="n">B</span><span class="o">*</span><span class="n">x</span><span class="x">)</span> <span class="n">≈</span> <span class="n">vcat</span><span class="x">(</span><span class="n">h_blks</span><span class="o">...</span><span class="x">)</span>        <span class="c"># Checking hidden states</span>
<span class="nd">@test</span> <span class="n">C</span><span class="err">'</span><span class="o">*</span><span class="x">(</span><span class="n">Ai</span><span class="o">*</span><span class="x">(</span><span class="n">B</span><span class="o">*</span><span class="n">x</span><span class="x">))</span> <span class="n">≈</span> <span class="n">vcat</span><span class="x">(</span><span class="n">y_blks</span><span class="o">...</span><span class="x">)</span>   <span class="c"># Checking measurement</span>
</code></pre></div></div> </details> <h2 id="state-space-models-as-smas">State-space models as SMAs</h2> <p>In this section we aim to show derive that the simplified SSM in the mamba-2 paper is a special case of Structured Masked Attention (SMA) <d-cite key="dao2024a"></d-cite>. That is that the multiplication</p> \[\bM\bx = \blkdiag(\bC_0, \dots, \bC_{T-1})^\top (\bA^{-1} (\blkdiag(\bB_0, \dots, \bB_{T-1}) \bx)),\] <p>can be written differently as</p> \[\bM\bx = \left(\bB\odot \bL\right) \bx,\] <p>The SSM in the Mamba-2 paper restricts the dynamics of $\bA$ to be scalar-times-identity dynamics in order for the masked to be structured <d-cite key="dao2024a"></d-cite>. In short this restriction mean that the dynamics for all hidden states are independent but equal.</p> \[\begin{aligned} \bh_i &amp;= \left(a_i\bI\right) \bh_{i-1} + \bb_i x_i, \\ \by_i &amp;= \bc_i^\top \bh_i \end{aligned}.\] <p>From a practical point-of-view this mean that we can collect each index of the dynamics and treat them separately. The resulting $\bA$-matrix can be described by a Kronecker product (depending on how we organize the states its either $\bA = \ba \otimes \bI_n$ or $\bA = \bI_n \otimes \ba$). In the following we choose to separate the states, resulting in $\bA$ having the form</p> \[\bA = \bI_n \otimes \ba, \quad \ba = \begin{bmatrix} 1 &amp; &amp; &amp; \\ -a_1 &amp; 1 &amp; &amp; \\ &amp; \ddots &amp; \ddots &amp; \\ &amp; &amp; -a_{T-1} &amp; 1 \end{bmatrix},\] <p>Using that the inverse of a Kronecker product is the Kronecker product of the inverses it follows that</p> \[\bA^{-1} = \bI_n \otimes \ba^{-1}, \quad \ba^{-1} = \tril\left( \underbrace{\begin{bmatrix}1 \\ a_1 \\ \vdots \\ a_{T-1}\dots a_1\end{bmatrix}}_{\bu} \underbrace{\begin{bmatrix}1 \\ a_1^{-1} \\ \vdots \\ (a_{T-1}\dots a_1)^{-1}\end{bmatrix}^\top}_{\bv^\top}\right),\] <p>where we further used that the inverse of a bidiagonal matrix is semiseparable. Furthermore, we have to re-arrange $\bB$ and $\bC$ which result in</p> \[\bB = \begin{bmatrix} \diag\left(\bb_1\right) \\ \vdots \\ \diag\left(\bb_n\right)\end{bmatrix}, \quad \bb_i = \begin{bmatrix} b_1^{(i)} \\ \vdots \\ b_T^{(i)} \end{bmatrix},\quad \bC = \begin{bmatrix} \diag\left(\bc_1\right) \\ \vdots \\ \diag\left(\bc_n\right)\end{bmatrix}, \quad \bc_i = \begin{bmatrix} c_1^{(i)} \\ \vdots \\ c_T^{(i)} \end{bmatrix}.\] <p>The final multiplication will therefore look as</p> \[\underbrace{\bC^\top \bA^{-1} \bB}_{\bM}\bx = \left(\sum_{i=1}^n \diag(\bc_i) \ba^{-1} \diag(\bb_i)\right)\bx = \left(\sum_{i=1}^n\left(\bc_i\bb_i^\top\right)\odot \ba^{-1}\right)\bx.\] <p>Finally, using the properties of the Hadamard product we get to the Structured Masked Attention form that we were looking for</p> \[\left(\bC^\top \bA^{-1} \bB \right)\bx = \left(\sum_{i=1}^n\left(\bc_i\bb_i^\top\right)\odot \ba^{-1}\right)\bx = \left(\left(\begin{bmatrix} \bc_1 &amp; \dots &amp; \bc_n \end{bmatrix} \begin{bmatrix} \bb_1^\top \\ \vdots \\ \bb_n^\top \end{bmatrix}\right)\odot \ba^{-1}\right)\bx.\] <p>This mean that the SSM dynamics can be interpreted as a structured masked attention mechanism. Note that in the case of the dynamics being independent but different (i.e we would have $\ba_i$ rather than just $\ba$) the SSM dynamics would result in a sum of masked attentions, i.e</p> \[\left(\bC^\top \bA^{-1} \bB \right)\bx = \left(\sum_{i=1}^n\left(\bc_i\bb_i^\top\right)\odot \ba_i^{-1}\right)\bx\] <details><summary>Code</summary> <div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">LinearAlgebra</span><span class="x">,</span> <span class="n">Test</span><span class="x">,</span> <span class="n">SymSemiseparableMatrices</span>

<span class="n">T</span> <span class="o">=</span> <span class="mi">10</span>          <span class="c"># Sequence length</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">6</span>           <span class="c"># State size</span>

<span class="c"># Here we treat the matrices in terms of their states and not sequence lengths</span>
<span class="n">a_blks</span> <span class="o">=</span> <span class="n">rand</span><span class="x">(</span><span class="n">T</span><span class="o">-</span><span class="mi">1</span><span class="x">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="kt">Bidiagonal</span><span class="x">(</span><span class="n">ones</span><span class="x">(</span><span class="n">T</span><span class="x">),</span><span class="o">-</span><span class="n">a_blks</span><span class="x">,</span><span class="o">:</span><span class="n">L</span><span class="x">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">kron</span><span class="x">(</span><span class="n">I</span><span class="x">(</span><span class="n">n</span><span class="x">),</span><span class="n">a</span><span class="x">)</span>        <span class="c"># Kronecker product with identity</span>

<span class="c"># The blocks are now size equal to the sequence and a block for each state!</span>
<span class="n">B_blks</span> <span class="o">=</span> <span class="x">[</span><span class="n">rand</span><span class="x">(</span><span class="n">T</span><span class="x">)</span> <span class="k">for</span> <span class="n">_</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">n</span><span class="x">]</span>
<span class="n">C_blks</span> <span class="o">=</span> <span class="x">[</span><span class="n">rand</span><span class="x">(</span><span class="n">T</span><span class="x">)</span> <span class="k">for</span> <span class="n">_</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">n</span><span class="x">]</span>  
<span class="c"># Collecting the blocks into the B and C</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">vcat</span><span class="x">(</span><span class="kt">Diagonal</span><span class="o">.</span><span class="x">(</span><span class="n">B_blks</span><span class="x">)</span><span class="o">...</span><span class="x">)</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">vcat</span><span class="x">(</span><span class="kt">Diagonal</span><span class="o">.</span><span class="x">(</span><span class="n">C_blks</span><span class="x">)</span><span class="o">...</span><span class="x">)</span> 

<span class="c"># We want to see if the full matrix M = C'*(A\B) can be written as</span>
<span class="c"># structured masked attention ie. </span>
<span class="c"># M = (CB')\circ a^{-1}. For this we start by computing a^{-1}</span>
<span class="n">ai</span> <span class="o">=</span> <span class="n">inv</span><span class="x">(</span><span class="n">a</span><span class="x">)</span> <span class="c"># We ignore here that inv(a) is semiseparable</span>
<span class="nd">@test</span> <span class="n">C</span><span class="err">'</span><span class="o">*</span><span class="x">(</span><span class="n">A</span><span class="o">\</span><span class="n">B</span><span class="x">)</span> <span class="n">≈</span> <span class="n">sum</span><span class="x">(</span><span class="n">i</span><span class="o">-&gt;</span> <span class="kt">Diagonal</span><span class="x">(</span><span class="n">C_blks</span><span class="x">[</span><span class="n">i</span><span class="x">])</span><span class="o">*</span><span class="n">ai</span><span class="o">*</span><span class="kt">Diagonal</span><span class="x">(</span><span class="n">B_blks</span><span class="x">[</span><span class="n">i</span><span class="x">]),</span><span class="mi">1</span><span class="o">:</span><span class="n">n</span><span class="x">)</span>
<span class="nd">@test</span> <span class="n">C</span><span class="err">'</span><span class="o">*</span><span class="x">(</span><span class="n">A</span><span class="o">\</span><span class="n">B</span><span class="x">)</span> <span class="n">≈</span> <span class="n">sum</span><span class="x">(</span><span class="n">i</span><span class="o">-&gt;</span><span class="x">(</span><span class="n">C_blks</span><span class="x">[</span><span class="n">i</span><span class="x">]</span><span class="o">*</span><span class="n">B_blks</span><span class="x">[</span><span class="n">i</span><span class="x">]</span><span class="err">'</span><span class="x">)</span> <span class="o">.*</span> <span class="n">ai</span> <span class="x">,</span> <span class="mi">1</span><span class="o">:</span><span class="n">n</span><span class="x">)</span>

<span class="c"># We can collect the terms and write is as Structured Masked Attention!</span>
<span class="n">Cn</span> <span class="o">=</span> <span class="n">hcat</span><span class="x">(</span><span class="n">C_blks</span><span class="o">...</span><span class="x">)</span>
<span class="n">Bn</span> <span class="o">=</span> <span class="n">hcat</span><span class="x">(</span><span class="n">B_blks</span><span class="o">...</span><span class="x">)</span>
<span class="nd">@test</span> <span class="n">C</span><span class="err">'</span><span class="o">*</span><span class="x">(</span><span class="n">A</span><span class="o">\</span><span class="n">B</span><span class="x">)</span> <span class="n">≈</span> <span class="x">(</span><span class="n">Cn</span><span class="o">*</span><span class="n">Bn</span><span class="err">'</span><span class="x">)</span><span class="o">.*</span><span class="n">ai</span>

<span class="c"># We can apply the semiseparable structure of the inverse when multiplying!</span>
<span class="n">ai_blks</span> <span class="o">=</span> <span class="x">[</span><span class="n">prod</span><span class="x">(</span><span class="n">a_blks</span><span class="x">[</span><span class="mi">1</span><span class="o">:</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="x">],</span><span class="n">init</span><span class="o">=</span><span class="mf">1.0</span><span class="x">)</span> <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">T</span><span class="x">]</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">vcat</span><span class="x">(</span><span class="n">ai_blks</span><span class="o">...</span><span class="x">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">vcat</span><span class="x">(</span><span class="n">inv</span><span class="o">.</span><span class="x">(</span><span class="n">ai_blks</span><span class="x">)</span><span class="o">...</span><span class="x">)</span>
<span class="c"># "SymSemiseparableCholesky" represents the matrix A^{-1} = tril(UV') </span>
<span class="n">ais</span> <span class="o">=</span> <span class="n">SymSemiseparableCholesky</span><span class="x">(</span><span class="n">u</span><span class="err">'</span><span class="x">,</span><span class="n">v</span><span class="err">'</span><span class="x">)</span>
<span class="c"># Efficient products using the structure of "ais" and diagonal B and C</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">randn</span><span class="x">(</span><span class="n">T</span><span class="x">)</span>
<span class="nd">@test</span> <span class="n">C</span><span class="err">'</span><span class="o">*</span><span class="x">(</span><span class="n">A</span><span class="o">\</span><span class="x">(</span><span class="n">B</span><span class="o">*</span><span class="n">x</span><span class="x">))</span> <span class="n">≈</span> <span class="n">sum</span><span class="x">(</span><span class="n">i</span><span class="o">-&gt;</span> <span class="n">C_blks</span><span class="x">[</span><span class="n">i</span><span class="x">]</span><span class="o">.*</span><span class="x">(</span><span class="n">ais</span><span class="o">*</span><span class="x">(</span><span class="n">B_blks</span><span class="x">[</span><span class="n">i</span><span class="x">]</span><span class="o">.*</span><span class="n">x</span><span class="x">)),</span><span class="mi">1</span><span class="o">:</span><span class="n">n</span><span class="x">)</span>
</code></pre></div></div> </details> ]]></content><author><name>Mikkel Paltorp</name></author><category term="attention"/><category term="linear-algebra"/><category term="state-space-models"/><category term="semiseparable-matrices"/><summary type="html"><![CDATA[and its relation to semiseparable matrices and state-space models]]></summary></entry><entry><title type="html">Continuous Matrix Factorizations</title><link href="https://mipals.github.io/blog/2024/continuous-matrix-factorization/" rel="alternate" type="text/html" title="Continuous Matrix Factorizations"/><published>2024-05-20T12:00:00+00:00</published><updated>2024-05-20T12:00:00+00:00</updated><id>https://mipals.github.io/blog/2024/continuous-matrix-factorization</id><content type="html" xml:base="https://mipals.github.io/blog/2024/continuous-matrix-factorization/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>This note was inspired by the plenary talk by Joel A. Tropp at SIAM LA24. While I could not find the specific talk online a very similar talk was recorded at the <a href="https://www.youtube.com/watch?v=A9D8KT6N1-8">Boeing Colloquium Series</a>. In the talk Joel presented the work of <d-cite key="chen2024a"></d-cite> , and I will in this note make the relation between that work and that on continuous matrix factorizations by Alex Townsend <d-cite key="townsend2015a"></d-cite>.</p> <h2 id="the-discrete-cholesky-factorization">The (Discrete) Cholesky Factorization</h2> <p>The Cholesky factorization of a positive semidefinite matrix $\mathbf{K}$ is given by</p> \[\mathbf{K} = \mathbf{L}\mathbf{L}^\top \in \mathbb{R}^{n\times n},\] <p>where $\mathbf{L}$ is a lower triangular matrix. Where $\mathbf{L}$ and $\mathbf{L}^\top$ are the factors of interest. The factorization is useful as it can be used to solve the linear systems $\mathbf{K}\mathbf{s} =\mathbf{t}$ by performing two triangular solves ($\mathbf{L}\mathbf{y} = \mathbf{t}, \mathbf{L}^\top\mathbf{s} = \mathbf{y}$). The Cholesky factorization can be computed by performing $n$ rank-1-updates of the original matrix. The first iterate is</p> \[\mathbf{K}^{(0)} = \begin{bmatrix}k &amp; \mathbf{m}^\top \\ \mathbf{m} &amp; \mathbf{M}\end{bmatrix} = \frac{1}{k}\begin{bmatrix}k \\ \mathbf{m}\end{bmatrix}\begin{bmatrix}k \\ \mathbf{m}\end{bmatrix}^\top + \underbrace{\begin{bmatrix} 0 &amp; 0 \\0 &amp; \mathbf{M} - \mathbf{m}\mathbf{m}^\top /k\end{bmatrix}}_{\text{Residual. Denoted by } \mathbf{K}^{(1)}}.\] <p>Each iteration eliminates a row and a column of the <em>residual</em> $\mathbf{K}^{(i)}$, with the first residual being the matrix itself. Thus after $r$ rank-1-update we have eliminated $r$ rows and columns of $\mathbf{K}$. The rows and columns that gets eliminated at the $i$th iteration is called the $i$th <em>pivot</em>. After $n$ iterations every row and column have been eliminated and we have the full factorization.</p> <p>In this note we will mostly be interested in the <em>partial</em> Cholesky factorization, which corresponds to stopping the Cholesky factorization after $r$ iterations. The partial Cholesky factorization will be a rank $r$ approximation of $\mathbf{K}$. The aim is that if $r \ll n$, then the approximation is a “data efficient” representation of $\mathbf{K}$. There exist various of approaches of how to chose the pivots <d-cite key="townsend2015a"></d-cite></p> <details><summary>Pivoting strategies</summary> <p>Let $\mathcal{I}_i$ be the unpicked columns after $i$ iterations. Then we can pick the next pivots as</p> <ul> <li> <p>Greedy: Pick the Next pivot element on the diagonal \(s_i = \text{argmax}_{k\in \mathcal{I}_i}\ \mathbf{A}^{(i-1)}(k,k)\)</p> </li> <li> <p>Uniform: Pick uniformly \(s_i \sim \text{uniform}\{\mathcal{I}_i\}\)</p> </li> <li> <p>Random pivoting: Pick with probability proportional to the diagonal element \(\mathbb{P}\left\{s_i = j\right\} = \frac{\mathbf{A}^{(i-1)}(j,j)}{\text{tr}\ \mathbf{A}^{(i-1)}}, \quad \forall j = 1,\dots,N\)</p> </li> </ul> <p>Note that the above strategies are all the same family of choosing the pivots w.r.t. the Gibbs distribution (the above can be achieved by $\beta \in \lbrace\infty,0,1\rbrace$)</p> \[\mathbb{P}\left\{s_i = j\right\} = \frac{|\mathbf{A}^{(i-1)}(j,j)|^\beta}{\sum_{k=1}^N |\mathbf{A}^{(i-1)}(k,k)|^\beta}, \quad \forall j = 1,\dots,N\] </details> <p>In <d-cite key="chen2024a"></d-cite> it is argued that the randomized approach works best. But why? An intuitive explanation is that this approach does not depend on the ordering of the data which in some cases are benificial.</p> <h2 id="kernel-matrices-and-kernel-functions">Kernel Matrices and Kernel Functions</h2> <p>The motivation for the note is that of Gaussian process regression. A key concept in Gaussian process regression is that of a kernel matrix. The kernel matrix, $\mathbf{K}$, is a matrix whose elements are generated from a so-called kernel function $k(\mathbf{x},\mathbf{y})$.</p> \[\mathbf{K}_{ij} = k(\mathbf{x}_i, \mathbf{x}_j),\ \text{ where }\ \mathbf{x}_i, \mathbf{x}_j \in X\] <p>where $X = \lbrace\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_n \rbrace$ is a collection of data points. An important property of the kernel function is that it is positive semidefinite</p> \[k(\mathbf{x},\mathbf{x}) \geq 0,\] <p>which has the consequence of the resulting kernel matrix $\mathbf{K}$ is positive semidefinite. A commonly used kernel function is the squared exponential given by</p> \[k(\mathbf{x},\mathbf{x}) = \exp\left(-\frac{\|\mathbf{x}_i - \mathbf{x}_j\|_2^2}{2\ell}\right),\] <p>where $\ell$ is commonly referred to as the length scale.</p> <p>To illustrate the process we simulate data points $\mathbf{x}_i \in [-2,2]$ and plot both the discrete / observed kernel matrix $\mathbf{K}$ and the continuos kernel function. Two things to note: First that the $y-$axes below are flipped to conform to the standard kernel matrix form and secondly that the data is sorted, giving some structure to the kernel matrix.</p> <div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">LinearAlgebra</span><span class="x">,</span> <span class="n">StatsBase</span><span class="x">,</span> <span class="n">Plots</span><span class="x">,</span> <span class="n">Random</span><span class="x">,</span> <span class="n">InvertedIndices</span>
<span class="n">Random</span><span class="o">.</span><span class="n">seed!</span><span class="x">(</span><span class="mi">1234</span><span class="x">)</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c"># Number of data points</span>
<span class="n">k</span> <span class="o">=</span> <span class="mi">5</span>   <span class="c"># Rank of approximation</span>
<span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span> <span class="c"># Dimension of data</span>
<span class="n">l</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="c"># Lengthscale parameter</span>

<span class="n">X</span> <span class="o">=</span> <span class="x">[</span><span class="n">rand</span><span class="x">(</span><span class="n">floor</span><span class="x">(</span><span class="kt">Int</span><span class="x">,</span><span class="n">n</span><span class="o">/</span><span class="mi">3</span><span class="x">),</span><span class="n">dim</span><span class="x">)</span> <span class="o">.-</span> <span class="mi">2</span><span class="x">;</span> <span class="n">rand</span><span class="x">(</span><span class="n">floor</span><span class="x">(</span><span class="kt">Int</span><span class="x">,</span><span class="n">n</span><span class="o">/</span><span class="mi">3</span><span class="x">),</span><span class="n">dim</span><span class="x">);</span> <span class="n">rand</span><span class="x">(</span><span class="n">floor</span><span class="x">(</span><span class="kt">Int</span><span class="x">,</span><span class="n">n</span><span class="o">/</span><span class="mi">3</span><span class="x">),</span><span class="n">dim</span><span class="x">)</span><span class="o">/</span><span class="mi">2</span> <span class="o">.+</span> <span class="mf">1.5</span><span class="x">]</span> <span class="c"># Three groups</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">length</span><span class="x">(</span><span class="n">X</span><span class="x">)</span> <span class="c"># Number of data points</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sort</span><span class="x">(</span><span class="n">X</span><span class="x">[</span><span class="o">:</span><span class="x">])</span> <span class="c"># Soring the data - Not strictly necessary, but makes the discrete form nicer</span>
<span class="n">G</span><span class="x">(</span><span class="n">x</span><span class="x">,</span><span class="n">y</span><span class="x">,</span><span class="n">l</span><span class="o">=</span><span class="n">l</span><span class="x">)</span> <span class="o">=</span> <span class="n">exp</span><span class="x">(</span><span class="o">-</span><span class="n">norm</span><span class="x">(</span><span class="n">x</span><span class="o">-</span><span class="n">y</span><span class="x">)</span><span class="o">^</span><span class="mi">2</span><span class="o">/</span><span class="x">(</span><span class="mi">2</span><span class="o">*</span><span class="n">l</span><span class="x">))</span>

<span class="c"># The actual view is in 2D</span>
<span class="n">o</span> <span class="o">=</span> <span class="n">ones</span><span class="x">(</span><span class="n">n</span><span class="x">)</span>
<span class="n">Xx</span> <span class="o">=</span> <span class="n">kron</span><span class="x">(</span><span class="n">X</span><span class="x">,</span><span class="n">o</span><span class="x">)</span>
<span class="n">Xy</span> <span class="o">=</span> <span class="n">kron</span><span class="x">(</span><span class="n">o</span><span class="x">,</span><span class="n">X</span><span class="x">)</span>
<span class="n">Gk</span> <span class="o">=</span> <span class="x">[</span><span class="n">G</span><span class="x">(</span><span class="n">x</span><span class="x">,</span><span class="n">y</span><span class="x">)</span> <span class="k">for</span> <span class="x">(</span><span class="n">x</span><span class="x">,</span><span class="n">y</span><span class="x">)</span> <span class="k">in</span> <span class="n">zip</span><span class="x">(</span><span class="n">Xx</span><span class="x">,</span><span class="n">Xy</span><span class="x">)]</span>
<span class="n">plot_matrix</span> <span class="o">=</span> <span class="n">heatmap</span><span class="x">(</span><span class="n">sort</span><span class="x">(</span><span class="n">X</span><span class="x">[</span><span class="o">:</span><span class="x">]),</span><span class="n">sort</span><span class="x">(</span><span class="n">X</span><span class="x">[</span><span class="o">:</span><span class="x">]),</span><span class="n">reshape</span><span class="x">(</span><span class="n">Gk</span><span class="x">,</span><span class="n">n</span><span class="x">,</span><span class="n">n</span><span class="x">),</span><span class="n">aspect_ratio</span><span class="o">=:</span><span class="n">equal</span><span class="x">,</span> <span class="n">title</span><span class="o">=</span><span class="s">"Discrete"</span><span class="x">)</span>
<span class="n">scatter!</span><span class="x">(</span><span class="n">plot_matrix</span><span class="x">,</span><span class="n">Xx</span><span class="x">,</span><span class="n">Xy</span><span class="x">,</span> <span class="n">label</span><span class="o">=</span><span class="nb">false</span><span class="x">)</span>
<span class="n">xlabel!</span><span class="x">(</span><span class="n">plot_matrix</span><span class="x">,</span><span class="s">"x"</span><span class="x">);</span> <span class="n">ylabel!</span><span class="x">(</span><span class="n">plot_matrix</span><span class="x">,</span><span class="s">"y"</span><span class="x">);</span> <span class="n">yflip!</span><span class="x">(</span><span class="nb">true</span><span class="x">)</span>

<span class="n">Xc</span> <span class="o">=</span> <span class="n">range</span><span class="x">(</span><span class="o">-</span><span class="mi">2</span><span class="x">,</span><span class="mi">2</span><span class="x">,</span><span class="mi">300</span><span class="x">)</span>
<span class="n">plot_smooth</span> <span class="o">=</span> <span class="n">contour</span><span class="x">(</span><span class="n">Xc</span><span class="x">,</span><span class="n">Xc</span><span class="x">,</span> <span class="x">(</span><span class="n">x</span><span class="x">,</span><span class="n">y</span><span class="x">)</span> <span class="o">-&gt;</span> <span class="n">G</span><span class="x">(</span><span class="n">x</span><span class="x">,</span><span class="n">y</span><span class="x">),</span> <span class="n">fill</span><span class="o">=</span><span class="nb">true</span><span class="x">,</span><span class="n">aspect_ratio</span><span class="o">=:</span><span class="n">equal</span><span class="x">,</span><span class="n">clim</span><span class="o">=</span><span class="x">(</span><span class="mi">0</span><span class="x">,</span><span class="mi">1</span><span class="x">))</span>
<span class="n">xlabel!</span><span class="x">(</span><span class="n">plot_smooth</span><span class="x">,</span><span class="s">"x"</span><span class="x">);</span> <span class="n">ylabel!</span><span class="x">(</span><span class="n">plot_smooth</span><span class="x">,</span><span class="s">"y"</span><span class="x">);</span> <span class="n">yflip!</span><span class="x">(</span><span class="nb">true</span><span class="x">)</span>
<span class="n">scatter!</span><span class="x">(</span><span class="n">plot_smooth</span><span class="x">,</span><span class="n">Xx</span><span class="x">,</span><span class="n">Xy</span><span class="x">,</span> <span class="n">label</span><span class="o">=</span><span class="nb">false</span><span class="x">,</span><span class="n">title</span><span class="o">=</span><span class="s">"Continuous"</span><span class="x">)</span>
<span class="n">scatter!</span><span class="x">(</span><span class="n">plot_smooth</span><span class="x">,</span><span class="n">X</span><span class="x">,</span><span class="n">X</span><span class="x">,</span> <span class="n">label</span><span class="o">=</span><span class="nb">false</span><span class="x">,</span> <span class="n">color</span><span class="o">=:</span><span class="n">black</span><span class="x">)</span>
<span class="n">plot</span><span class="x">(</span><span class="n">plot_matrix</span><span class="x">,</span><span class="n">plot_smooth</span><span class="x">,</span> <span class="n">layour</span><span class="o">=</span><span class="x">(</span><span class="mi">1</span><span class="x">,</span><span class="mi">2</span><span class="x">),</span><span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="x">)</span>
</code></pre></div></div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/initial_scatter-480.webp 480w,/assets/img/continuous/initial_scatter-800.webp 800w,/assets/img/continuous/initial_scatter-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/continuous/initial_scatter.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="the-continuous-cholesky-factorization">The Continuous Cholesky Factorization</h2> <p>Approximating the discrete form with a low-rank matrix is as easy performing a partial Cholesky. However, a different approach is needed in order to approximate the continuous form of the “low-rank” function. In <d-cite key="townsend2015a"></d-cite> continuous analogous to matrix factorization is introduced in the context of so-called <em>cmatrices</em>. These matrices can be represented as function of two continuous variables, i.e. that they are elements of $C([a,b]\times[c,d])$. While the properties is only stated for scalar $x$ and $y$ one can easily extend the ideas for a general bivariate function (such as a kernel function).</p> <p>The continuous Cholesky factorization of rank $r$ of kernel function is given as</p> \[k(\mathbf{x},\mathbf{y}) \approx \sum_{i=1}^{r}\frac{k_i(\mathbf{x},\mathbf{x}_i)k_i(\mathbf{x}_i, \mathbf{y})}{k_i(\mathbf{x}_i, \mathbf{x}_i)} = \sum_{i=1}^{r}\frac{k_i(\mathbf{x},\mathbf{x}_i)}{\sqrt{k_i(\mathbf{x}_i, \mathbf{x}_i)}}\frac{k_i(\mathbf{x}_i, \mathbf{y})}{\sqrt{k_i(\mathbf{x}_i, \mathbf{x}_i)}}\] <p>where $(\mathbf{x}_i,\mathbf{x}_i)$ are the so-called pivot points and</p> \[k_i(\mathbf{x},\mathbf{y}) = \begin{cases} k(\mathbf{x},\mathbf{y}) \quad &amp;i = 1\\ k_{i-1}(\mathbf{x},\mathbf{y}) - \frac{k_{i-1}(\mathbf{x},\mathbf{x}_{i-1})k_{i-1}(\mathbf{x}_{i-1},\mathbf{y})}{k_{i-1}(\mathbf{x}_{i-1},\mathbf{x}_{i-1})}\quad &amp;i \geq 2. \end{cases}\] <h2 id="visual-representations">Visual Representations</h2> <h3 id="the-greedy-approach">The Greedy Approach</h3> <p>As the initial residual is just the matrix itself $(\mathbf{K}^{(0)})$ then its diagonal is filled with ones. As such the greedy method will simply pick the first pivot depending on the ordering of the data. This is one of the weaknesses of the greedy method, namely that is susceptible to bad ordering of the data. In fact <d-cite key="chen2024a"></d-cite> gives an example full of outliers. On this example the greedy approach completely fails and gets stuck picking the pivots equal to the ordering of the data. Note that had the ordering of the data in <d-cite key="chen2024a"></d-cite> been random, then implicitly the greedy method would not have failed.</p> <p>In the previously data introduces is completely ordered w.r.t. $x$, and the greedy method therefore picks the first pivot as the point with smallest value. The next iteration again start by looking at the diagonal, but this time of the residual $\mathbf{K}^{(1)}$. Given that the first pivot was chosen as smallest value, it is no surprise that the worst approximation happens furthest away. As such the next pivot point will be at the largest value. The two iterations highlight the weakness of the greedy method: In most cases it ends up picking pivots on the borders of the dataset.</p> <p>The series of the first 4 iterations can be seen below.</p> <h4 id="1st-iteration">1st iteration</h4> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank0_diag-480.webp 480w,/assets/img/continuous/rank0_diag-800.webp 800w,/assets/img/continuous/rank0_diag-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/continuous/rank0_diag.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank1_approx-480.webp 480w,/assets/img/continuous/rank1_approx-800.webp 800w,/assets/img/continuous/rank1_approx-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/continuous/rank1_approx.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="2nd-iteration">2nd iteration</h4> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank1_diag-480.webp 480w,/assets/img/continuous/rank1_diag-800.webp 800w,/assets/img/continuous/rank1_diag-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/continuous/rank1_diag.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank2_approx-480.webp 480w,/assets/img/continuous/rank2_approx-800.webp 800w,/assets/img/continuous/rank2_approx-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/continuous/rank2_approx.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="3rd-iteration">3rd iteration</h4> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank2_diag-480.webp 480w,/assets/img/continuous/rank2_diag-800.webp 800w,/assets/img/continuous/rank2_diag-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/continuous/rank2_diag.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank3_approx-480.webp 480w,/assets/img/continuous/rank3_approx-800.webp 800w,/assets/img/continuous/rank3_approx-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/continuous/rank3_approx.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="4th-iteration">4th iteration</h4> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank3_diag-480.webp 480w,/assets/img/continuous/rank3_diag-800.webp 800w,/assets/img/continuous/rank3_diag-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/continuous/rank3_diag.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank4_approx-480.webp 480w,/assets/img/continuous/rank4_approx-800.webp 800w,/assets/img/continuous/rank4_approx-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/continuous/rank4_approx.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="the-randomized-approach">The Randomized Approach</h3> <p>We now repeat the first 4 steps, but using randomized pivoting approach. A noticeable difference between the greedy and the randomized approach is that the randomized approach prioritizes (with some probability off course) to pick points close to where the data actually is. This is a win, as a point “inside” of the dataset will reduce the overall diagonal of the residual in the places of where data is actually located. In addition, the approach is also completely robust against the ordering of the data, as a randomization of the data is built directly into the approach.</p> <h4 id="1st-iteration-1">1st iteration</h4> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank0_diag_random-480.webp 480w,/assets/img/continuous/rank0_diag_random-800.webp 800w,/assets/img/continuous/rank0_diag_random-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/continuous/rank0_diag_random.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank1_approx_random-480.webp 480w,/assets/img/continuous/rank1_approx_random-800.webp 800w,/assets/img/continuous/rank1_approx_random-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/continuous/rank1_approx_random.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="2nd-iteration-1">2nd iteration</h4> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank1_diag_random-480.webp 480w,/assets/img/continuous/rank1_diag_random-800.webp 800w,/assets/img/continuous/rank1_diag_random-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/continuous/rank1_diag_random.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank2_approx_random-480.webp 480w,/assets/img/continuous/rank2_approx_random-800.webp 800w,/assets/img/continuous/rank2_approx_random-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/continuous/rank2_approx_random.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="3rd-iteration-1">3rd iteration</h4> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank2_diag_random-480.webp 480w,/assets/img/continuous/rank2_diag_random-800.webp 800w,/assets/img/continuous/rank2_diag_random-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/continuous/rank2_diag_random.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank3_approx_random-480.webp 480w,/assets/img/continuous/rank3_approx_random-800.webp 800w,/assets/img/continuous/rank3_approx_random-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/continuous/rank3_approx_random.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="4th-iteration-1">4th iteration</h4> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank3_diag_random-480.webp 480w,/assets/img/continuous/rank3_diag_random-800.webp 800w,/assets/img/continuous/rank3_diag_random-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/continuous/rank3_diag_random.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank4_approx_random-480.webp 480w,/assets/img/continuous/rank4_approx_random-800.webp 800w,/assets/img/continuous/rank4_approx_random-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/continuous/rank4_approx_random.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="comparing-diagonals">Comparing diagonals</h3> <p>To highlight the differences between the two approaches we plot the diagonals together. The overall decay of the diagonal is similar for each method. However, the residual within the dataset is clearly smaller for the randomized approach as compared to the greedy approach. A downside of the randomized approach is that the approximation is not as good “outside” of the data. Said differently, the randomized approach generalizes worse than the greedy approach, by relying on pivots inside, rather than the border, of the dataset.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank1_diags-480.webp 480w,/assets/img/continuous/rank1_diags-800.webp 800w,/assets/img/continuous/rank1_diags-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/continuous/rank1_diags.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank2_diags-480.webp 480w,/assets/img/continuous/rank2_diags-800.webp 800w,/assets/img/continuous/rank2_diags-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/continuous/rank2_diags.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank3_diags-480.webp 480w,/assets/img/continuous/rank3_diags-800.webp 800w,/assets/img/continuous/rank3_diags-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/continuous/rank3_diags.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/continuous/rank4_diags-480.webp 480w,/assets/img/continuous/rank4_diags-800.webp 800w,/assets/img/continuous/rank4_diags-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/continuous/rank4_diags.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="final-remarks">Final remarks</h2> <p>A thing to note about the above is that while the approach only uses pivot points that are in dataset, this not a requirement. That is, a better pivot point could be found outside the dataset. This would be similar to the inducing point approach in sparse Gaussian processes <d-cite key="inducingpoints"></d-cite>, where the inducing points do not have to be part of the dataset.</p>]]></content><author><name>Mikkel Paltorp</name></author><category term="linear-algebra"/><category term="gaussian-processes"/><category term="low-rank-approximation"/><category term="randomized-linear-algebra"/><summary type="html"><![CDATA[and their relation to kernel approximations]]></summary></entry></feed>